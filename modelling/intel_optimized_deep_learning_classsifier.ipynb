{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Optimizing Neural Networks with Intel oneAPI and PyTorch Extensions\n","This notebook demonstrates the process of optimizing neural networks using PyTorch, Intel oneAPI, and Intel Extension for PyTorch (IPEX). "]},{"cell_type":"code","execution_count":9,"id":"ebe50dad-0c98-4e72-815f-37720be64440","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["2.1.2\n","1.13.1+cpu\n","1.13.100\n","0.14.1+cpu\n"]}],"source":["# Import necessary libraries for computation and deep learning\n","import numpy as np  # Numerical computing library\n","import torch  # PyTorch for building and training neural networks\n","import intel_extension_for_pytorch as ipex  # Intel extension to optimize PyTorch models on Intel hardware\n","import torchvision  # Useful for computer vision tasks, though not used directly in this notebook\n","\n","# Print the versions of each library to verify compatibility\n","print(np.__version__)  # Numpy version\n","print(torch.__version__)  # PyTorch version\n","print(ipex.__version__)  # Intel Extension for PyTorch version\n","print(torchvision.__version__)  # Torchvision version\n"]},{"cell_type":"markdown","id":"98f7d059","metadata":{},"source":["## Importing Necessary Libraries\n","**Torch**: The core library for PyTorch, enabling tensor operations and GPU support for deep learning.\n","\n","**torch.nn**: Contains classes and functions for building and managing neural networks, including layers and activation functions. Used for defining the `ImprovedNN` model.\n","\n","**torch.optim**: Provides optimization algorithms like SGD and **Adam**(used in our case) to update model parameters during training. Used for defining the optimizer in the training process.\n","\n","**Learning Rate Scheduler**: Dynamically adjusts the learning rate during training to improve convergence. Used for reducing the learning rate when a plateau in loss is detected.\n","\n","**Scikit-learn Metrics**: Evaluation metrics for classification tasks, helping assess model performance. Used for calculating accuracy, precision, recall, F1-score, AUC, and balanced accuracy.\n","\n","**train_test_split**: Utility function for splitting the dataset into training and testing subsets. Used in the `data_splitter` function to split the data.\n","\n","**classification_report**: Generates a report detailing precision, recall, F1-score, and support for each class. Not directly used in the notebook but imported for potential use.\n","\n","**DataLoader**: Facilitates efficient batching and loading of datasets. Used for creating data loaders for training and testing datasets.\n","\n","**TensorDataset**: Wraps tensors into a dataset format for compatibility with DataLoader. Used for converting the data into a format compatible with DataLoader.\n","\n","**NumPy**: A library for numerical computing, providing support for arrays and mathematical functions. Used for various numerical operations and data manipulation.\n","\n","**Pandas**: A data manipulation library that simplifies data analysis and handling of tabular data. Used for loading and preprocessing the dataset.\n","\n","**tqdm**: Library for creating progress bars, enhancing user experience during long computations. Not directly used in the notebook but imported for potential use.\n","\n","**Intel Extension for PyTorch (IPEX)**: This library optimizes the performance of PyTorch models specifically on Intel hardware, providing enhanced execution speed through optimized mathematical operations and efficient memory usage. Used for optimizing the PyTorch models.\n"]},{"cell_type":"code","execution_count":null,"id":"5613f81a","metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, balanced_accuracy_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","from torch.utils.data import DataLoader, TensorDataset\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import intel_extension_for_pytorch as ipex"]},{"cell_type":"markdown","id":"d3e40258","metadata":{},"source":["## Loading and Preparing Data for Modeling\n","**Loading the Dataset**: The dataset is read from a specified CSV file using Pandas’ read_csv() function. This file is expected to contain various attributes related to cities, including information on EV stations and other civic amenities.\n","\n","**Error Handling**: A try-except block is utilized to catch potential FileNotFoundError. This ensures that if the file is not found at the specified path, a clear message is printed to guide the user in troubleshooting.\n","\n","**Error Handling**: A try-except block is utilized to catch potential FileNotFoundError. This ensures that if the file is not found at the specified path, a clear message is printed to guide the user in troubleshooting.\n","\n","**Column Filtering**: This line filters the DataFrame to retain only the relevant columns necessary for modeling. The selected features encompass various aspects of urban infrastructure and population, which are likely predictors for the target variable.\n","\n","**Initial Data Size**: The shape of the DataFrame is printed, giving a quick overview of the number of rows (samples) and columns (features) in the dataset before any preprocessing steps.\n","\n","**Dropping Missing Values**: This line removes any rows containing missing values (NaN) from the DataFrame. Handling missing values is critical to ensure the dataset is clean and usable for model training, as many algorithms do not accept missing data.\n","\n","**Post-cleaning Data Size**: After removing rows with missing values, the new shape of the DataFrame is printed. This provides insight into how much data has been retained and how the preprocessing has affected the dataset’s size.\n"]},{"cell_type":"code","execution_count":null,"id":"eede3ff3","metadata":{},"outputs":[],"source":["\n","try:\n","    data = pd.read_csv(\"../datasets/processed/all_city_data_with_pop.csv\")\n","    print(\"Data loaded successfully!\")\n","except FileNotFoundError as e:\n","    print(\"File not found. Please check the path:\", e)\n","# Filter columns to be used for modeling\n","data = data[['geometry', 'city', 'EV_stations', 'parking', 'edges',\n","             'parking_space', 'civic', 'restaurant', 'park', 'school',\n","             'node', 'Community_centre', 'place_of_worship', 'university', 'cinema',\n","             'library', 'commercial', 'retail', 'townhall', 'government',\n","             'residential', 'population']]\n","\n","print(\"Data size:\", data.shape)\n","data = data.dropna()\n","print(\"Data size after dropping na:\", data.shape)"]},{"cell_type":"code","execution_count":null,"id":"45817854","metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, balanced_accuracy_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","from torch.utils.data import DataLoader, TensorDataset\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import intel_extension_for_pytorch as ipex\n","\n","try:\n","    data = pd.read_csv(\"/home/ucce21c3bd21d7702289d768bb49aa80/Training/AI/Introduction_to_PyTorch_24/all_city_data_with_pop.csv\")\n","    print(\"Data loaded successfully!\")\n","except FileNotFoundError as e:\n","    print(\"File not found. Please check the path:\", e)\n","# Filter columns to be used for modeling\n","data = data[['geometry', 'city', 'EV_stations', 'parking', 'edges',\n","             'parking_space', 'civic', 'restaurant', 'park', 'school',\n","             'node', 'Community_centre', 'place_of_worship', 'university', 'cinema',\n","             'library', 'commercial', 'retail', 'townhall', 'government',\n","             'residential', 'population']]\n","\n","print(\"Data size:\", data.shape)\n","data = data.dropna()\n","print(\"Data size after dropping na:\", data.shape)"]},{"cell_type":"markdown","id":"0666604f","metadata":{},"source":["## data_splitter: splits the dataset into training and testing sets based on the input cities\n","It can handle two scenarios:\n","\n","\t•\tIf specific train_cities and test_cities are provided, it filters the dataset accordingly.\n","\t•\tIf no cities are specified, it uses train_test_split to split the data into training and test sets randomly based on the given test_size.\n","\n","Parameters:\n","\n","\t•\tdata: The complete dataset.\n","\t•\ttrain_cities: List of cities to be included in the training set (optional).\n","\t•\ttest_cities: List of cities to be included in the test set (optional).\n","\t•\ttest_size: Proportion of the data to include in the test split.\n","\t•\trandom_state: Random seed for reproducibility.\n","\n","Returns:\n","\n","\t•\tX_train: Features of the training set.\n","\t•\tX_test: Features of the test set.\n","\t•\ty_train: Target labels for the training set.\n","\t•\ty_test: Target labels for the test set."]},{"cell_type":"code","execution_count":null,"id":"bc4df632","metadata":{},"outputs":[],"source":["def data_splitter(data, train_cities=None, test_cities=None, test_size=0.2, random_state=42):\n","    if train_cities is not None:\n","        train = data[data['city'].isin(train_cities)]\n","        test = data[data['city'].isin(test_cities)]\n","\n","        X_train = train.drop(['city', 'geometry', 'EV_stations'], axis=1)\n","        y_train = train['EV_stations'].astype(int)\n","        y_train = y_train.apply(lambda x: 1 if x > 0 else 0)\n","\n","        X_test = test.drop(['city', 'geometry', 'EV_stations'], axis=1)\n","        y_test = test['EV_stations'].astype(int)\n","        y_test = y_test.apply(lambda x: 1 if x > 0 else 0)\n","    else:\n","        X = data.drop(['city', 'geometry', 'EV_stations'], axis=1)\n","        y = data['EV_stations']\n","        y = y.apply(lambda x: 1 if x > 0 else 0)\n","        X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=test_size, random_state=random_state)\n","\n","    return X_train, X_test, y_train, y_test\n","\n","X_train, X_test, y_train, y_test = data_splitter(data)"]},{"cell_type":"markdown","id":"11923d4c","metadata":{},"source":["This block converts the X_train, X_test, y_train, and y_test datasets from Pandas DataFrames into PyTorch tensors. Tensors are the core data structures in PyTorch used to train the neural network.\n","\n","\t•\tThe .unsqueeze(1) operation reshapes the labels (y_train and y_test) to have a second dimension, making it compatible with the neural network’s output shape."]},{"cell_type":"code","execution_count":null,"id":"aa9974f5","metadata":{},"outputs":[],"source":["X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n","X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n","y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)\n","y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)"]},{"cell_type":"markdown","id":"141ae593","metadata":{},"source":["## ImprovedNN :A Neural Network Model with Batch Normalization and Dropout\n","\n","This class defines a custom neural network model. It inherits from nn.Module, the base class for all neural network modules in PyTorch.\n","\n","Architecture:\n","\n","\t•\tfc1: The first fully connected layer with 256 units.\n","\t•\tbn1: Batch normalization applied after the first layer.\n","\t•\tdropout1: Dropout layer to prevent overfitting by randomly deactivating neurons during training.\n","\t•\tfc2: Second fully connected layer with 128 units.\n","\t•\tbn2: Batch normalization applied after the second layer.\n","\t•\tdropout2: Dropout layer.\n","\t•\tfc3: Third fully connected layer with 64 units.\n","\t•\tfc4: Output layer with a single unit using sigmoid activation to produce a probability output for binary classification.\n","\n","The activation function used between layers is ReLU for non-linearity, and sigmoid is applied at the output to restrict predictions between 0 and 1."]},{"cell_type":"code","execution_count":null,"id":"8490609b","metadata":{},"outputs":[],"source":["class ImprovedNN(nn.Module):\n","    def __init__(self):\n","        super(ImprovedNN, self).__init__()\n","        self.fc1 = nn.Linear(X_train.shape[1], 256)\n","        self.bn1 = nn.BatchNorm1d(256)\n","        self.dropout1 = nn.Dropout(0.5)\n","        self.fc2 = nn.Linear(256, 128)\n","        self.bn2 = nn.BatchNorm1d(128)\n","        self.dropout2 = nn.Dropout(0.5)\n","        self.fc3 = nn.Linear(128, 64)\n","        self.fc4 = nn.Linear(64, 1)\n","\n","    def forward(self, x):\n","        x = torch.relu(self.bn1(self.fc1(x)))\n","        x = self.dropout1(x)\n","        x = torch.relu(self.bn2(self.fc2(x)))\n","        x = self.dropout2(x)\n","        x = torch.relu(self.fc3(x))\n","        x = torch.sigmoid(self.fc4(x))\n","        return x"]},{"cell_type":"markdown","id":"bf8bc7b3","metadata":{},"source":["## Early Stopping Mechanism: Technique to prevent overfitting during training\n","Early stopping is a technique to prevent overfitting during training. If the model’s loss does not improve after a certain number of epochs, training is stopped.\n","\n","Attributes:\n","\n","\t•\tpatience: The number of epochs to wait for an improvement in loss before stopping training.\n","\t•\tmin_delta: Minimum change in the monitored loss to qualify as an improvement.\n","\n","Functionality:\n","\n","\t•\tstep(loss): Monitors the loss at each epoch and stops the training if it doesn’t improve after patience epochs.\n"]},{"cell_type":"code","execution_count":null,"id":"68f90539","metadata":{},"outputs":[],"source":["class EarlyStopping:\n","    def __init__(self, patience=5, min_delta=0.01):\n","        self.patience = patience\n","        self.min_delta = min_delta\n","        self.best_loss = None\n","        self.counter = 0\n","\n","    def step(self, loss):\n","        if self.best_loss is None:\n","            self.best_loss = loss\n","        elif self.best_loss - loss > self.min_delta:\n","            self.best_loss = loss\n","            self.counter = 0\n","        else:\n","            self.counter += 1\n","\n","        if self.counter >= self.patience:\n","            return True\n","        return False"]},{"cell_type":"markdown","id":"76cbf52a","metadata":{},"source":[]},{"cell_type":"markdown","id":"d61c4a21","metadata":{},"source":["## Weight Initialization:Xavier (Glorot) initialization\n","This function applies Xavier (Glorot) initialization to the weights of the linear layers. Proper weight initialization can help with better convergence during training."]},{"cell_type":"code","execution_count":null,"id":"42dd0623","metadata":{},"outputs":[],"source":["def weights_init(m):\n","    if isinstance(m, nn.Linear):\n","        nn.init.xavier_uniform_(m.weight)\n","        nn.init.zeros_(m.bias)\n"]},{"cell_type":"markdown","id":"dfa58c6c","metadata":{},"source":["## Training Function: Handles the training process of the neural network\n","This function handles the training process of the neural network.\n","\n","Parameters:\n","\n","\t•\tmodel: The neural network model to be trained.\n","\t•\toptimizer: The optimization algorithm (Adam in this case) that updates the model’s weights.\n","\t•\tcriterion: The loss function (Binary Cross-Entropy).\n","\t•\tscheduler: Learning rate scheduler to adjust the learning rate based on the loss.\n","\t•\tearly_stopping: Early stopping instance to monitor loss and stop training early if necessary.\n","\t•\ttrain_loader: DataLoader for the training data.\n","\t•\tval_loader: DataLoader for the validation data.\n","\t•\tepochs: Number of training epochs.\n","\t•\tdevice: The device to run the model on (CPU or GPU).\n","\n","Process:\n","\n","\t•\tThe function loops through each epoch, performs a forward pass, calculates the loss, performs backpropagation, and updates the weights.\n","\t•\tIt also implements gradient clipping to prevent exploding gradients.\n","\t•\tThe scheduler adjusts the learning rate based on the loss at the end of each epoch.\n","\t•\tEarly stopping is checked, and if triggered, training stops early."]},{"cell_type":"markdown","id":"9227975d","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"id":"682adfff","metadata":{},"outputs":[],"source":["# Training Function\n","def train_model(model, optimizer, criterion, scheduler, early_stopping, train_loader, val_loader, epochs=50, device='cpu'):\n","    model.train()\n","    for epoch in range(epochs):\n","        running_loss = 0.0\n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            # Forward pass\n","            outputs = model(inputs.float())\n","            loss = criterion(outputs, labels.unsqueeze(1).float())\n","\n","            # Backward pass and optimization\n","            optimizer.zero_grad()\n","            loss.backward()\n","\n","            # Gradient clipping\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n","\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","\n","        # Scheduler step\n","        scheduler.step(running_loss)\n","\n","        # Early Stopping check\n","        if early_stopping.step(running_loss):\n","            print(f\"Early stopping at epoch {epoch}\")\n","            break\n","\n","        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss:.4f}\")\n","\n","    return model"]},{"cell_type":"markdown","id":"0bc6e419","metadata":{},"source":["## Model Evaluation Function:Evaluates the trained model on the test dataset &&  Optimizing Performance with Intel Extension for PyTorch (IPEX)\n","**Function Definition**: This function evaluates the performance of a given neural network model on a test dataset.\n","\t•\tParameters:\n","\t•\tmodel: The trained PyTorch model to be evaluated.\n","\t•\ttest_loader: A DataLoader object that provides the test dataset in batches.\n","\t•\tdevice: The device to run the model on ('cpu' or 'cuda'). Defaults to 'cpu'.\n","\n","**Set Model to Evaluation Mode**: This method sets the model to evaluation mode, which disables dropout and batch normalization, ensuring consistent outputs.\n","\n","**Model Optimization**: Uses **Intel Extension for PyTorch (IPEX)** to **optimize the model** for better performance on Intel hardware.\n","\n","**Return True and Predicted Labels**: Converts both lists of true labels and predictions into NumPy arrays and returns them, allowing for further analysis of the model’s performance."]},{"cell_type":"code","execution_count":null,"id":"28067f4a","metadata":{},"outputs":[],"source":["def evaluate_model(model, test_loader, device='cpu'):\n","    model.eval()\n","    model=ipex.optimize(model)\n","    y_true = []\n","    y_pred = []\n","\n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            inputs = inputs.to(device)\n","            outputs = model(inputs.float())\n","            predictions = (outputs > 0.5).float().cpu().numpy()\n","            y_pred.extend(predictions)\n","            y_true.extend(labels.cpu().numpy())\n","\n","    return np.array(y_true), np.array(y_pred)"]},{"cell_type":"markdown","id":"3a6a883b","metadata":{},"source":["## run_experiment:Sets up and runs the entire experiment\n","This is the main function that sets up and runs the entire experiment.\n","\n","Process:\n","\n","\t•\tThe data is first converted into PyTorch tensors and wrapped into DataLoader objects for batching.\n","\t•\tA neural network model (ImprovedNN) is instantiated and initialized with Xavier initialization.\n","\t•\tThe optimizer, loss function, learning rate scheduler, and early stopping mechanism are defined.\n","\t•\tThe model is trained using the train_model function.\n","\t•\tAfter training, the model is evaluated using the evaluate_model function.\n","\n","Evaluation Metrics:\n","\n","\t•\tAccuracy: The proportion of correct predictions.\n","\t•\tPrecision: The number of true positives divided by the number of predicted positives.\n","\t•\tRecall: The number of true positives divided by the number of actual positives.\n","\t•\tF1-score: The harmonic mean of precision and recall.\n","\t•\tAUC: Area under the ROC curve, a performance measure for binary classification.\n","\t•\tBalanced Accuracy: Accuracy adjusted for imbalanced datasets."]},{"cell_type":"code","execution_count":null,"id":"4543360b","metadata":{},"outputs":[],"source":["def run_experiment(X_train, X_test, y_train, y_test, batch_size=32, epochs=50):\n","    # Convert Pandas DataFrames to NumPy arrays first\n","    X_train_np = X_train.values\n","    X_test_np = X_test.values\n","    y_train_np = y_train.values\n","    y_test_np = y_test.values\n","\n","    # Convert data to PyTorch tensors and create DataLoader\n","    train_dataset = TensorDataset(torch.tensor(X_train_np), torch.tensor(y_train_np))\n","    test_dataset = TensorDataset(torch.tensor(X_test_np), torch.tensor(y_test_np))\n","\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n","\n","    # Define the device (use GPU if available)\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    # Initialize model, optimizer, and loss function\n","    model = ImprovedNN().to(device)\n","    model.apply(weights_init)\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","    criterion = nn.BCELoss()\n","\n","    # Define Learning Rate Scheduler and Early Stopping\n","    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, verbose=True)\n","    early_stopping = EarlyStopping(patience=50)\n","\n","    # Train the model\n","    model = train_model(model, optimizer, criterion, scheduler, early_stopping, train_loader, test_loader, epochs=epochs, device=device)\n","\n","    # Evaluate the model\n","    y_true, y_pred = evaluate_model(model, test_loader, device=device)\n","\n","    # Calculate evaluation metrics\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred, average='macro')\n","    recall = recall_score(y_true, y_pred, average='macro')\n","    f1 = f1_score(y_true, y_pred, average='macro')\n","    auc = roc_auc_score(y_true, y_pred)\n","    balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n","\n","    # Return metrics and the trained model\n","    return {\n","        \"Accuracy\": accuracy,\n","        \"Precision\": precision,\n","        \"Recall\": recall,\n","        \"F1-score\": f1,\n","        \"AUC\": auc,\n","        \"Balanced Accuracy\": balanced_accuracy\n","    }, model\n","\n","\n","metrics, trained_model = run_experiment(X_train, X_test, y_train, y_test)\n","print(metrics)"]},{"cell_type":"markdown","id":"368660dd","metadata":{},"source":["Data loaded successfully!\n","Data size: (10824, 22)\n","Data size after dropping na: (10129, 22)\n","Epoch 1/50, Loss: 80.4817\n","Epoch 2/50, Loss: 68.2528\n","Epoch 3/50, Loss: 67.3190\n","Epoch 4/50, Loss: 66.2678\n","Epoch 5/50, Loss: 65.5271\n","Epoch 6/50, Loss: 64.1401\n","Epoch 7/50, Loss: 64.5227\n","Epoch 8/50, Loss: 63.6566\n","Epoch 10/50, Loss: 63.7573\n","Epoch 11/50, Loss: 62.5218\n","Epoch 12/50, Loss: 63.7462\n","Epoch 13/50, Loss: 62.7203\n","Epoch 14/50, Loss: 63.1687\n","Epoch 16/50, Loss: 62.6809\n","Epoch 00017: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 17/50, Loss: 62.8853\n","Epoch 18/50, Loss: 62.5329\n","Epoch 19/50, Loss: 62.2598\n","Epoch 20/50, Loss: 62.0549\n","Epoch 21/50, Loss: 61.8264\n","Epoch 22/50, Loss: 62.0168\n","Epoch 23/50, Loss: 61.5555\n","Epoch 24/50, Loss: 61.6650\n","Epoch 25/50, Loss: 61.9277\n","Epoch 26/50, Loss: 60.9742\n","Epoch 27/50, Loss: 61.2287\n","Epoch 28/50, Loss: 61.3921\n","Epoch 29/50, Loss: 61.8382\n","Epoch 30/50, Loss: 62.0283\n","Epoch 31/50, Loss: 61.1148\n","Epoch 00032: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 32/50, Loss: 61.9968\n","Epoch 33/50, Loss: 61.4239\n","Epoch 35/50, Loss: 61.5319\n","Epoch 36/50, Loss: 61.1627\n","Epoch 37/50, Loss: 61.4312\n","Epoch 00038: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 38/50, Loss: 61.4890\n","Epoch 39/50, Loss: 61.9879\n","Epoch 40/50, Loss: 60.0656\n","Epoch 41/50, Loss: 61.6155\n","Epoch 42/50, Loss: 60.9621\n","Epoch 43/50, Loss: 60.5539\n","Epoch 44/50, Loss: 61.1476\n","Epoch 45/50, Loss: 61.1083\n","Epoch 47/50, Loss: 61.4583\n","Epoch 48/50, Loss: 61.3163\n","Epoch 49/50, Loss: 60.7892\n","Epoch 50/50, Loss: 61.1086\n","{'Accuracy': 0.903751233958539, 'Precision': 0.8501829109833214, 'Recall': 0.6226040500186636, 'F1-score': 0.6663246044113362, 'AUC': 0.6226040500186637, 'Balanced Accuracy': 0.6226040500186636}\n"]}],"metadata":{"kernelspec":{"display_name":"oneAPI Latest","language":"python","name":"oneapi-latest"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":5}
