{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of the imports:\n",
    "- **pandas**: For handling data in a tabular format (DataFrames), making it easy to manipulate and analyze datasets.\n",
    "- **numpy**: For efficient numerical computations on large arrays and matrices.\n",
    "- **matplotlib.pyplot**: For generating plots and visualizing data (such as ROC curves, feature distributions, etc.).\n",
    "- **sklearn.model_selection.train_test_split**: Used to split the data into training and testing sets.\n",
    "- **sklearn.linear_model.LogisticRegression**: This class implements logistic regression for binary classification problems.\n",
    "- **sklearn.metrics.classification_report**: Generates a detailed report on classification performance, including precision, recall, F1-score, etc.\n",
    "- **tqdm.notebook.tqdm**: Provides a progress bar for loops, useful in Jupyter notebooks when running long processes.\n",
    "- **sklearn.metrics**: Various functions like accuracy, precision, recall, F1-score, ROC-AUC, and balanced accuracy are used to evaluate the performance of the model.\n",
    "- **sklearn.utils.all_estimators**: A utility function to retrieve all available estimators in scikit-learn.\n",
    "- **modin.pandas**: A drop-in replacement for pandas, enabling parallel and faster DataFrame operations, utilizing multiple CPU cores.\n",
    "- **sklearnex**: Intel’s extension of scikit-learn that optimizes various algorithms for better performance, particularly on Intel architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries             # For data manipulation\n",
    "import numpy as np                # For numerical operations\n",
    "import matplotlib.pyplot as plt   # For data visualization\n",
    "\n",
    "# Machine learning model and evaluation imports\n",
    "from sklearn.model_selection import train_test_split  # For splitting the dataset into training and testing sets\n",
    "from sklearn.linear_model import LogisticRegression   # For logistic regression model\n",
    "from sklearn.metrics import classification_report     # For detailed classification metrics\n",
    "\n",
    "# Progress bar for loops\n",
    "from tqdm.notebook import tqdm   # For progress bar visualization in Jupyter notebooks\n",
    "\n",
    "# Additional metrics for evaluating model performance\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, balanced_accuracy_score\n",
    "\n",
    "# Utility function to get all estimators from scikit-learn\n",
    "from sklearn.utils import all_estimators\n",
    "\n",
    "import modin.pandas as pd  # Import Modin for parallel DataFrame operations\n",
    "\n",
    "from sklearnex import patch_sklearn  # Import sklearnex for scikit-learn optimization\n",
    "\n",
    "patch_sklearn()  # Patch scikit-learn with sklearnex\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset from Project Root Directory\n",
    "\n",
    "In this section, we set the root directory for the project and load the dataset into a pandas DataFrame. The dataset contains processed city data, including population details.\n",
    "\n",
    "#### Variables:\n",
    "- `path_root_dir`: The root directory path where the datasets are stored.\n",
    "- `data`: The DataFrame that holds the loaded CSV data.\n",
    "\n",
    "The `pd.read_csv()` function is used to read the CSV file into the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set this to the root directory of the project\n",
    "path_root_dir=\"../datasets/\"\n",
    "data = pd.read_csv(path_root_dir + \"processed/all_city_data_with_pop.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>geometry</th>\n",
       "      <th>parking</th>\n",
       "      <th>edges</th>\n",
       "      <th>EV_stations</th>\n",
       "      <th>parking_space</th>\n",
       "      <th>civic</th>\n",
       "      <th>restaurant</th>\n",
       "      <th>park</th>\n",
       "      <th>...</th>\n",
       "      <th>cinema</th>\n",
       "      <th>library</th>\n",
       "      <th>commercial</th>\n",
       "      <th>retail</th>\n",
       "      <th>townhall</th>\n",
       "      <th>government</th>\n",
       "      <th>residential</th>\n",
       "      <th>city</th>\n",
       "      <th>population</th>\n",
       "      <th>Berlin_data_onlycenter_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>POLYGON ((8.4727605 50.099822499999995, 8.4730...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Frankfurt</td>\n",
       "      <td>9.014051</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>POLYGON ((8.4775730092433 50.10302720327834, 8...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Frankfurt</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>POLYGON ((8.479750879173663 50.09863320231676,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Frankfurt</td>\n",
       "      <td>9.014051</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>POLYGON ((8.479688060978736 50.10443297769501,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Frankfurt</td>\n",
       "      <td>9.014051</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>POLYGON ((8.47965547981383 50.107440331063444,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Frankfurt</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0   \n",
       "0             0           0  \\\n",
       "1             1           1   \n",
       "2             2           2   \n",
       "3             3           3   \n",
       "4             4           4   \n",
       "\n",
       "                                            geometry  parking  edges   \n",
       "0  POLYGON ((8.4727605 50.099822499999995, 8.4730...        0      0  \\\n",
       "1  POLYGON ((8.4775730092433 50.10302720327834, 8...        0      0   \n",
       "2  POLYGON ((8.479750879173663 50.09863320231676,...        0      0   \n",
       "3  POLYGON ((8.479688060978736 50.10443297769501,...        0      0   \n",
       "4  POLYGON ((8.47965547981383 50.107440331063444,...        0      0   \n",
       "\n",
       "   EV_stations  parking_space  civic  restaurant  park  ...  cinema  library   \n",
       "0            0              0      0           0     0  ...       0        0  \\\n",
       "1            0              0      0           0     0  ...       0        0   \n",
       "2            0              0      0           0     0  ...       0        0   \n",
       "3            0              0      0           0     0  ...       0        0   \n",
       "4            0              0      0           0     0  ...       0        0   \n",
       "\n",
       "   commercial  retail  townhall  government  residential       city   \n",
       "0           0       0         0         0.0            0  Frankfurt  \\\n",
       "1           0       0         0         0.0            0  Frankfurt   \n",
       "2           0       0         0         0.0            0  Frankfurt   \n",
       "3           0       0         0         0.0            0  Frankfurt   \n",
       "4           0       0         0         0.0            0  Frankfurt   \n",
       "\n",
       "   population  Berlin_data_onlycenter_  \n",
       "0    9.014051                      NaN  \n",
       "1    0.000000                      NaN  \n",
       "2    9.014051                      NaN  \n",
       "3    9.014051                      NaN  \n",
       "4    0.000000                      NaN  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Relevant Columns for Modeling\n",
    "\n",
    "Here, we filter out the necessary columns from the dataset for building our machine learning model. These columns contain important features such as the number of EV stations, parking, schools, population, and various other civic, commercial, and residential attributes.\n",
    "\n",
    "- **Columns Selected for Modeling**:\n",
    "  - `geometry`, `city`, `EV_stations`, `parking`, `edges`, `parking_space`, `civic`, `restaurant`, `park`, `school`, `node`, `Community_centre`, `place_of_worship`, `university`, `cinema`, `library`, `commercial`, `retail`, `townhall`, `government`, `residential`, `population`.\n",
    "  \n",
    "- **Drop Missing Values**:\n",
    "  - After filtering out the columns, rows with missing values (NaN) are removed using `dropna()` to ensure clean data for modeling.\n",
    "\n",
    "#### Code to Filter and Clean Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size: (10824, 22)\n",
      "data size after dropping na: (10129, 22)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geometry</th>\n",
       "      <th>city</th>\n",
       "      <th>EV_stations</th>\n",
       "      <th>parking</th>\n",
       "      <th>edges</th>\n",
       "      <th>parking_space</th>\n",
       "      <th>civic</th>\n",
       "      <th>restaurant</th>\n",
       "      <th>park</th>\n",
       "      <th>school</th>\n",
       "      <th>...</th>\n",
       "      <th>place_of_worship</th>\n",
       "      <th>university</th>\n",
       "      <th>cinema</th>\n",
       "      <th>library</th>\n",
       "      <th>commercial</th>\n",
       "      <th>retail</th>\n",
       "      <th>townhall</th>\n",
       "      <th>government</th>\n",
       "      <th>residential</th>\n",
       "      <th>population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POLYGON ((8.4727605 50.099822499999995, 8.4730...</td>\n",
       "      <td>Frankfurt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.014051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POLYGON ((8.4775730092433 50.10302720327834, 8...</td>\n",
       "      <td>Frankfurt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>POLYGON ((8.479750879173663 50.09863320231676,...</td>\n",
       "      <td>Frankfurt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.014051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>POLYGON ((8.479688060978736 50.10443297769501,...</td>\n",
       "      <td>Frankfurt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.014051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>POLYGON ((8.47965547981383 50.107440331063444,...</td>\n",
       "      <td>Frankfurt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            geometry       city  EV_stations   \n",
       "0  POLYGON ((8.4727605 50.099822499999995, 8.4730...  Frankfurt            0  \\\n",
       "1  POLYGON ((8.4775730092433 50.10302720327834, 8...  Frankfurt            0   \n",
       "2  POLYGON ((8.479750879173663 50.09863320231676,...  Frankfurt            0   \n",
       "3  POLYGON ((8.479688060978736 50.10443297769501,...  Frankfurt            0   \n",
       "4  POLYGON ((8.47965547981383 50.107440331063444,...  Frankfurt            0   \n",
       "\n",
       "   parking  edges  parking_space  civic  restaurant  park  school  ...   \n",
       "0        0      0              0      0           0     0       0  ...  \\\n",
       "1        0      0              0      0           0     0       0  ...   \n",
       "2        0      0              0      0           0     0       0  ...   \n",
       "3        0      0              0      0           0     0       0  ...   \n",
       "4        0      0              0      0           0     0       0  ...   \n",
       "\n",
       "   place_of_worship  university  cinema  library  commercial  retail   \n",
       "0                 0           0       0        0           0       0  \\\n",
       "1                 0           0       0        0           0       0   \n",
       "2                 0           0       0        0           0       0   \n",
       "3                 0           0       0        0           0       0   \n",
       "4                 0           0       0        0           0       0   \n",
       "\n",
       "   townhall  government  residential  population  \n",
       "0         0         0.0            0    9.014051  \n",
       "1         0         0.0            0    0.000000  \n",
       "2         0         0.0            0    9.014051  \n",
       "3         0         0.0            0    9.014051  \n",
       "4         0         0.0            0    0.000000  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering the relevant columns for modeling\n",
    "data = data[['geometry','city','EV_stations', 'parking', 'edges',\n",
    "        'parking_space', 'civic', 'restaurant', 'park', 'school',\n",
    "       'node', 'Community_centre', 'place_of_worship', 'university', 'cinema',\n",
    "       'library', 'commercial', 'retail', 'townhall', 'government',\n",
    "       'residential', 'population']]\n",
    "\n",
    "# Print initial data size\n",
    "print(\"data size:\", data.shape)\n",
    "\n",
    "# Drop rows with missing values\n",
    "data = data.dropna()\n",
    "\n",
    "# Print data size after dropping missing values\n",
    "print(\"data size after dropping na:\", data.shape)\n",
    "\n",
    "# Display the first few rows of the filtered and cleaned data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: `data_splitter`\n",
    "\n",
    "This function is designed to split the dataset into training and testing sets, either based on predefined cities (if provided) or by using random sampling when no city-specific splitting is required. The goal is to train a model that predicts the presence of EV stations in the cities. \n",
    "\n",
    "- **Two Modes of Splitting**:\n",
    "  - **City-based splitting**: If `train_cities` and `test_cities` are provided, the function will filter the dataset based on these cities.\n",
    "  - **Random-based splitting**: If no city-specific splitting is provided, the dataset will be split randomly using `train_test_split`.\n",
    "\n",
    "- **Binary Classification Target**:\n",
    "  - The target variable `EV_stations` is transformed into a binary variable (1 or 0), where `1` indicates the presence of EV stations in the city and `0` indicates none.\n",
    "\n",
    "#### Parameters:\n",
    "- **`data`**: The input DataFrame containing city and feature information.\n",
    "- **`train_cities`**: (Optional) A list of city names to be used for the training set.\n",
    "- **`test_cities`**: (Optional) A list of city names to be used for the test set.\n",
    "- **`test_size`**: The proportion of data to use for testing (default is 0.2, or 20%).\n",
    "- **`random_state`**: A seed value for reproducibility in random splits.\n",
    "\n",
    "#### Steps in the Code:\n",
    "1. **City-based Splitting**:\n",
    "    - If `train_cities` and `test_cities` are provided:\n",
    "      - Filter rows belonging to `train_cities` for training and `test_cities` for testing.\n",
    "      - Drop the `city`, `geometry`, and `EV_stations` columns from features.\n",
    "      - Apply binary transformation on `EV_stations` as the target variable (1 for presence, 0 for absence).\n",
    "      \n",
    "2. **Random-based Splitting**:\n",
    "    - If city lists are not provided:\n",
    "      - Use `train_test_split` to randomly split the data into training and testing sets, stratifying based on the target variable (`EV_stations`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_splitter(data, train_cities=None, test_cities=None, test_size=0.2, random_state=42):\n",
    "\n",
    "    if train_cities is not None:\n",
    "        train = data[data['city'].isin(train_cities)]\n",
    "        test = data[data['city'].isin(test_cities)]\n",
    "\n",
    "\n",
    "        X_train = train.drop(['city','geometry', 'EV_stations'], axis=1)\n",
    "        y_train = train['EV_stations'].astype(int)\n",
    "        y_train = y_train.apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "        X_test = test.drop(['city','geometry', 'EV_stations'], axis=1)\n",
    "        y_test = test['EV_stations'].astype(int)\n",
    "        y_test = y_test.apply(lambda x: 1 if x > 0 else 0)\n",
    "    else:\n",
    "        X = data.drop(['city','geometry', \"EV_stations\"], axis=1)\n",
    "        y = data['EV_stations']\n",
    "        y = y.apply(lambda x: 1 if x > 0 else 0)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = data_splitter(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model\n",
    "\n",
    "This code block demonstrates the training and evaluation of a Logistic Regression model for predicting the presence of EV stations in the dataset. It uses the data split earlier into training and testing sets.\n",
    "\n",
    "#### Steps:\n",
    "1. **Model Initialization**: \n",
    "   - A `LogisticRegression()` model is created using the default parameters.\n",
    "\n",
    "2. **Model Training**: \n",
    "   - The model is trained on the training dataset (`X_train`, `y_train`) using the `.fit()` method.\n",
    "   \n",
    "3. **Model Evaluation**:\n",
    "   - The model's performance is evaluated on the test dataset (`X_test`, `y_test`) using the `.score()` method, which returns the accuracy of the model.\n",
    "   \n",
    "4. **Classification Report**:\n",
    "   - After making predictions on the test set using the `.predict()` method, a detailed classification report is generated using `classification_report()`. This report includes:\n",
    "     - Precision: The ratio of correctly predicted positive observations to total predicted positives.\n",
    "     - Recall: The ratio of correctly predicted positive observations to all actual positives.\n",
    "     - F1-Score: The weighted average of Precision and Recall.\n",
    "     - Support: The number of true instances for each label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Test Accuracy:  0.8958538993089832\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.98      0.94      1786\n",
      "           1       0.62      0.30      0.41       240\n",
      "\n",
      "    accuracy                           0.90      2026\n",
      "   macro avg       0.77      0.64      0.68      2026\n",
      "weighted avg       0.88      0.90      0.88      2026\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Print Test Accuracy\n",
    "print(\"Logistic Regression Test Accuracy: \", logreg.score(X_test, y_test))\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Multiple Classification Models\n",
    "\n",
    "This code block demonstrates how to apply and evaluate all available classification models from `scikit-learn` using the `all_estimators()` function. Each model is trained on the training dataset and evaluated on the test dataset. Key metrics like accuracy, precision, recall, F1-score, AUC, and balanced accuracy are calculated and stored in a DataFrame for comparison.\n",
    "\n",
    "#### Steps:\n",
    "1. **Fetching Classifiers**:\n",
    "   - `all_estimators(type_filter='classifier')` returns all available classifier models in `scikit-learn`.\n",
    "   \n",
    "2. **Model Training and Evaluation**:\n",
    "   - Each classifier is instantiated and trained using the training set (`X_train`, `y_train`).\n",
    "   - Predictions (`y_pred`) are made on the test set (`X_test`).\n",
    "   \n",
    "3. **Metrics Calculation**:\n",
    "   - The following evaluation metrics are computed for each model:\n",
    "     - **Accuracy**: Proportion of correct predictions out of all predictions.\n",
    "     - **Precision**: Ratio of correctly predicted positives to total predicted positives, computed using `precision_score()`.\n",
    "     - **Recall**: Ratio of correctly predicted positives to all actual positives, computed using `recall_score()`.\n",
    "     - **F1-Score**: Harmonic mean of precision and recall, computed using `f1_score()`.\n",
    "     - **AUC (Area Under the ROC Curve)**: Measures the ability of the model to distinguish between classes, computed using `roc_auc_score()`.\n",
    "     - **Balanced Accuracy**: Average of recall obtained in each class, computed using `balanced_accuracy_score()`.\n",
    "   \n",
    "4. **Results Storage**:\n",
    "   - Results for each classifier, along with their corresponding metrics, are stored in the `results` list.\n",
    "   \n",
    "5. **DataFrame Creation**:\n",
    "   - A Pandas DataFrame is created from the results for easier visualization and sorting.\n",
    "   - The DataFrame is sorted by `F1-score` and `AUC` in descending order to rank the best models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34dcf146815e415583b8bc2953d27b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Model  Accuracy  Precision    Recall  F1-score   \n",
      "12  HistGradientBoostingClassifier  0.900296   0.767657  0.705403  0.730580  \\\n",
      "0               AdaBoostClassifier  0.900296   0.768031  0.703600  0.729434   \n",
      "25          RandomForestClassifier  0.904245   0.788820  0.689609  0.724794   \n",
      "11      GradientBoostingClassifier  0.899309   0.766435  0.694023  0.722009   \n",
      "15      LinearDiscriminantAnalysis  0.894373   0.750447  0.680403  0.707106   \n",
      "8             ExtraTreesClassifier  0.897828   0.765678  0.673346  0.705678   \n",
      "1                BaggingClassifier  0.892892   0.745251  0.679563  0.704936   \n",
      "9                       GaussianNB  0.866239   0.687214  0.707727  0.696568   \n",
      "20                   MultinomialNB  0.872162   0.693752  0.693052  0.693401   \n",
      "19                   MLPClassifier  0.900790   0.790903  0.651582  0.691288   \n",
      "4                     ComplementNB  0.855874   0.674136  0.712668  0.689998   \n",
      "24   QuadraticDiscriminantAnalysis  0.861797   0.677888  0.697994  0.687014   \n",
      "2                      BernoulliNB  0.814413   0.658937  0.777517  0.684522   \n",
      "28                   SGDClassifier  0.890918   0.740760  0.653196  0.682853   \n",
      "3           CalibratedClassifierCV  0.898322   0.780082  0.644772  0.682806   \n",
      "18            LogisticRegressionCV  0.896841   0.771005  0.645735  0.682214   \n",
      "7              ExtraTreeClassifier  0.858342   0.670175  0.688820  0.678653   \n",
      "16                       LinearSVC  0.790721   0.658556  0.812771  0.678205   \n",
      "17              LogisticRegression  0.895854   0.768226  0.639765  0.675930   \n",
      "21                 NearestCentroid  0.794176   0.654741  0.796697  0.675566   \n",
      "5           DecisionTreeClassifier  0.853899   0.659650  0.675481  0.666901   \n",
      "26                 RidgeClassifier  0.897828   0.791270  0.624655  0.663200   \n",
      "27               RidgeClassifierCV  0.897335   0.789299  0.622571  0.660651   \n",
      "10       GaussianProcessClassifier  0.862290   0.660018  0.640566  0.649223   \n",
      "23                      Perceptron  0.880059   0.694345  0.594739  0.619051   \n",
      "29                             SVC  0.891905   0.780540  0.583424  0.611760   \n",
      "22     PassiveAggressiveClassifier  0.885489   0.757335  0.538307  0.542842   \n",
      "13                LabelPropagation  0.848963   0.519835  0.508574  0.503470   \n",
      "14                  LabelSpreading  0.848963   0.519835  0.508574  0.503470   \n",
      "6                  DummyClassifier  0.881540   0.440770  0.500000  0.468520   \n",
      "\n",
      "         AUC  Balanced Accuracy  \n",
      "12  0.705403           0.705403  \n",
      "0   0.703600           0.703600  \n",
      "25  0.689609           0.689609  \n",
      "11  0.694023           0.694023  \n",
      "15  0.680403           0.680403  \n",
      "8   0.673346           0.673346  \n",
      "1   0.679563           0.679563  \n",
      "9   0.707727           0.707727  \n",
      "20  0.693052           0.693052  \n",
      "19  0.651582           0.651582  \n",
      "4   0.712668           0.712668  \n",
      "24  0.697994           0.697994  \n",
      "2   0.777517           0.777517  \n",
      "28  0.653196           0.653196  \n",
      "3   0.644772           0.644772  \n",
      "18  0.645735           0.645735  \n",
      "7   0.688820           0.688820  \n",
      "16  0.812771           0.812771  \n",
      "17  0.639765           0.639765  \n",
      "21  0.796697           0.796697  \n",
      "5   0.675481           0.675481  \n",
      "26  0.624655           0.624655  \n",
      "27  0.622571           0.622571  \n",
      "10  0.640566           0.640566  \n",
      "23  0.594739           0.594739  \n",
      "29  0.583424           0.583424  \n",
      "22  0.538307           0.538307  \n",
      "13  0.508574           0.508574  \n",
      "14  0.508574           0.508574  \n",
      "6   0.500000           0.500000  \n"
     ]
    }
   ],
   "source": [
    "# Get all classification model classes\n",
    "classifiers = all_estimators(type_filter='classifier')\n",
    "\n",
    "# Initialize result table\n",
    "results = []\n",
    "models = {}\n",
    "# Run models and collect results\n",
    "for name, ClassifierClass in tqdm(classifiers):\n",
    "    try:\n",
    "        # Initialize model\n",
    "        model = ClassifierClass()\n",
    "        model.fit(X_train, y_train)\n",
    "        models[name] = model\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='macro')\n",
    "        recall = recall_score(y_test, y_pred, average='macro')\n",
    "        f1 = f1_score(y_test, y_pred, average='macro')\n",
    "        auc = roc_auc_score(y_test, y_pred)\n",
    "        balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        # Append results\n",
    "        results.append([name, accuracy, precision, recall, f1, auc, balanced_accuracy])\n",
    "    except Exception as e:\n",
    "        print(\"\", end=\"\")\n",
    "\n",
    "# Create a DataFrame from results\n",
    "results_df = pd.DataFrame(results, columns=[\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-score\", \"AUC\", \"Balanced Accuracy\"])\n",
    "results_df = results_df.sort_values(by=['F1-score', 'AUC'], ascending=False)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: `run_experiment`\n",
    "\n",
    "This function automates the process of training and evaluating multiple classification models on the provided dataset. It compares performance metrics across different classifiers, facilitating easy model selection.\n",
    "\n",
    "#### Inputs:\n",
    "- **`X_train`**: Feature set for training the models.\n",
    "- **`X_test`**: Feature set for testing the models.\n",
    "- **`y_train`**: Target labels for training.\n",
    "- **`y_test`**: Target labels for testing.\n",
    "\n",
    "#### Process:\n",
    "1. **Fetch Classifiers**:\n",
    "   - Uses `all_estimators` from `scikit-learn` to fetch all available classifier models for experimentation.\n",
    "\n",
    "2. **Initialize Result Storage**:\n",
    "   - A list `results` is created to store the performance metrics.\n",
    "   - A dictionary `models` is initialized to store trained models for later use.\n",
    "\n",
    "3. **Train and Evaluate Models**:\n",
    "   - The function iterates through each classifier:\n",
    "     - It initializes the model using the `ClassifierClass`.\n",
    "     - The model is trained on `X_train` and `y_train`.\n",
    "     - Predictions are made on the `X_test` set.\n",
    "     - Performance metrics, including accuracy, precision, recall, F1-score, AUC, and balanced accuracy, are computed.\n",
    "   - Results are stored in the `results` list, and the trained model is saved in the `models` dictionary.\n",
    "\n",
    "4. **Error Handling**:\n",
    "   - Exceptions are caught and handled silently to avoid stopping the entire process if a particular model fails.\n",
    "\n",
    "5. **Results Compilation**:\n",
    "   - The results are converted into a Pandas DataFrame (`results_df`), and the table is sorted by F1-score and AUC in descending order to highlight the best-performing models.\n",
    "\n",
    "#### Outputs:\n",
    "- **`results_df`**: A Pandas DataFrame containing the performance metrics for each model.\n",
    "- **`models`**: A dictionary with the trained models accessible by model name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(X_train, X_test, y_train, y_test):\n",
    "    # Get all classification model classes\n",
    "    classifiers = all_estimators(type_filter='classifier')\n",
    "\n",
    "    # Initialize result table\n",
    "    results = []\n",
    "    models = {}\n",
    "    # Run models and collect results\n",
    "    for name, ClassifierClass in tqdm(classifiers):\n",
    "        try:\n",
    "            # Initialize model\n",
    "            model = ClassifierClass()\n",
    "            model.fit(X_train, y_train)\n",
    "            models[name] = model\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            precision = precision_score(y_test, y_pred, average='macro')\n",
    "            recall = recall_score(y_test, y_pred, average='macro')\n",
    "            f1 = f1_score(y_test, y_pred, average='macro')\n",
    "            auc = roc_auc_score(y_test, y_pred)\n",
    "            balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "            \n",
    "            # Append results\n",
    "            results.append([name, accuracy, precision, recall, f1, auc, balanced_accuracy])\n",
    "        except Exception as e:\n",
    "            print(\"\", end=\"\")\n",
    "\n",
    "    # Create a DataFrame from results\n",
    "    results_df = pd.DataFrame(results, columns=[\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-score\", \"AUC\", \"Balanced Accuracy\"])\n",
    "    results_df = results_df.sort_values(by=['F1-score', 'AUC'], ascending=False)\n",
    "    return results_df, models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8532a7aa802d4e0eb461b7b7aa096641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_df, models = run_experiment(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>HistGradientBoostingClassifier</td>\n",
       "      <td>0.900296</td>\n",
       "      <td>0.767657</td>\n",
       "      <td>0.705403</td>\n",
       "      <td>0.730580</td>\n",
       "      <td>0.705403</td>\n",
       "      <td>0.705403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>0.900296</td>\n",
       "      <td>0.768031</td>\n",
       "      <td>0.703600</td>\n",
       "      <td>0.729434</td>\n",
       "      <td>0.703600</td>\n",
       "      <td>0.703600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.904245</td>\n",
       "      <td>0.788820</td>\n",
       "      <td>0.689609</td>\n",
       "      <td>0.724794</td>\n",
       "      <td>0.689609</td>\n",
       "      <td>0.689609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>0.899309</td>\n",
       "      <td>0.766435</td>\n",
       "      <td>0.694023</td>\n",
       "      <td>0.722009</td>\n",
       "      <td>0.694023</td>\n",
       "      <td>0.694023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LinearDiscriminantAnalysis</td>\n",
       "      <td>0.894373</td>\n",
       "      <td>0.750447</td>\n",
       "      <td>0.680403</td>\n",
       "      <td>0.707106</td>\n",
       "      <td>0.680403</td>\n",
       "      <td>0.680403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>0.897828</td>\n",
       "      <td>0.765678</td>\n",
       "      <td>0.673346</td>\n",
       "      <td>0.705678</td>\n",
       "      <td>0.673346</td>\n",
       "      <td>0.673346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BaggingClassifier</td>\n",
       "      <td>0.892892</td>\n",
       "      <td>0.745251</td>\n",
       "      <td>0.679563</td>\n",
       "      <td>0.704936</td>\n",
       "      <td>0.679563</td>\n",
       "      <td>0.679563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>0.866239</td>\n",
       "      <td>0.687214</td>\n",
       "      <td>0.707727</td>\n",
       "      <td>0.696568</td>\n",
       "      <td>0.707727</td>\n",
       "      <td>0.707727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.872162</td>\n",
       "      <td>0.693752</td>\n",
       "      <td>0.693052</td>\n",
       "      <td>0.693401</td>\n",
       "      <td>0.693052</td>\n",
       "      <td>0.693052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>0.900790</td>\n",
       "      <td>0.790903</td>\n",
       "      <td>0.651582</td>\n",
       "      <td>0.691288</td>\n",
       "      <td>0.651582</td>\n",
       "      <td>0.651582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ComplementNB</td>\n",
       "      <td>0.855874</td>\n",
       "      <td>0.674136</td>\n",
       "      <td>0.712668</td>\n",
       "      <td>0.689998</td>\n",
       "      <td>0.712668</td>\n",
       "      <td>0.712668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>0.861797</td>\n",
       "      <td>0.677888</td>\n",
       "      <td>0.697994</td>\n",
       "      <td>0.687014</td>\n",
       "      <td>0.697994</td>\n",
       "      <td>0.697994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.814413</td>\n",
       "      <td>0.658937</td>\n",
       "      <td>0.777517</td>\n",
       "      <td>0.684522</td>\n",
       "      <td>0.777517</td>\n",
       "      <td>0.777517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>SGDClassifier</td>\n",
       "      <td>0.890918</td>\n",
       "      <td>0.740760</td>\n",
       "      <td>0.653196</td>\n",
       "      <td>0.682853</td>\n",
       "      <td>0.653196</td>\n",
       "      <td>0.653196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CalibratedClassifierCV</td>\n",
       "      <td>0.898322</td>\n",
       "      <td>0.780082</td>\n",
       "      <td>0.644772</td>\n",
       "      <td>0.682806</td>\n",
       "      <td>0.644772</td>\n",
       "      <td>0.644772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegressionCV</td>\n",
       "      <td>0.896841</td>\n",
       "      <td>0.771005</td>\n",
       "      <td>0.645735</td>\n",
       "      <td>0.682214</td>\n",
       "      <td>0.645735</td>\n",
       "      <td>0.645735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ExtraTreeClassifier</td>\n",
       "      <td>0.858342</td>\n",
       "      <td>0.670175</td>\n",
       "      <td>0.688820</td>\n",
       "      <td>0.678653</td>\n",
       "      <td>0.688820</td>\n",
       "      <td>0.688820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>0.790721</td>\n",
       "      <td>0.658556</td>\n",
       "      <td>0.812771</td>\n",
       "      <td>0.678205</td>\n",
       "      <td>0.812771</td>\n",
       "      <td>0.812771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.895854</td>\n",
       "      <td>0.768226</td>\n",
       "      <td>0.639765</td>\n",
       "      <td>0.675930</td>\n",
       "      <td>0.639765</td>\n",
       "      <td>0.639765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NearestCentroid</td>\n",
       "      <td>0.794176</td>\n",
       "      <td>0.654741</td>\n",
       "      <td>0.796697</td>\n",
       "      <td>0.675566</td>\n",
       "      <td>0.796697</td>\n",
       "      <td>0.796697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>0.853899</td>\n",
       "      <td>0.659650</td>\n",
       "      <td>0.675481</td>\n",
       "      <td>0.666901</td>\n",
       "      <td>0.675481</td>\n",
       "      <td>0.675481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>RidgeClassifier</td>\n",
       "      <td>0.897828</td>\n",
       "      <td>0.791270</td>\n",
       "      <td>0.624655</td>\n",
       "      <td>0.663200</td>\n",
       "      <td>0.624655</td>\n",
       "      <td>0.624655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>RidgeClassifierCV</td>\n",
       "      <td>0.897335</td>\n",
       "      <td>0.789299</td>\n",
       "      <td>0.622571</td>\n",
       "      <td>0.660651</td>\n",
       "      <td>0.622571</td>\n",
       "      <td>0.622571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GaussianProcessClassifier</td>\n",
       "      <td>0.862290</td>\n",
       "      <td>0.660018</td>\n",
       "      <td>0.640566</td>\n",
       "      <td>0.649223</td>\n",
       "      <td>0.640566</td>\n",
       "      <td>0.640566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>0.880059</td>\n",
       "      <td>0.694345</td>\n",
       "      <td>0.594739</td>\n",
       "      <td>0.619051</td>\n",
       "      <td>0.594739</td>\n",
       "      <td>0.594739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.891905</td>\n",
       "      <td>0.780540</td>\n",
       "      <td>0.583424</td>\n",
       "      <td>0.611760</td>\n",
       "      <td>0.583424</td>\n",
       "      <td>0.583424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>PassiveAggressiveClassifier</td>\n",
       "      <td>0.885489</td>\n",
       "      <td>0.757335</td>\n",
       "      <td>0.538307</td>\n",
       "      <td>0.542842</td>\n",
       "      <td>0.538307</td>\n",
       "      <td>0.538307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LabelPropagation</td>\n",
       "      <td>0.848963</td>\n",
       "      <td>0.519835</td>\n",
       "      <td>0.508574</td>\n",
       "      <td>0.503470</td>\n",
       "      <td>0.508574</td>\n",
       "      <td>0.508574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LabelSpreading</td>\n",
       "      <td>0.848963</td>\n",
       "      <td>0.519835</td>\n",
       "      <td>0.508574</td>\n",
       "      <td>0.503470</td>\n",
       "      <td>0.508574</td>\n",
       "      <td>0.508574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DummyClassifier</td>\n",
       "      <td>0.881540</td>\n",
       "      <td>0.440770</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.468520</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Model  Accuracy  Precision    Recall  F1-score   \n",
       "12  HistGradientBoostingClassifier  0.900296   0.767657  0.705403  0.730580  \\\n",
       "0               AdaBoostClassifier  0.900296   0.768031  0.703600  0.729434   \n",
       "25          RandomForestClassifier  0.904245   0.788820  0.689609  0.724794   \n",
       "11      GradientBoostingClassifier  0.899309   0.766435  0.694023  0.722009   \n",
       "15      LinearDiscriminantAnalysis  0.894373   0.750447  0.680403  0.707106   \n",
       "8             ExtraTreesClassifier  0.897828   0.765678  0.673346  0.705678   \n",
       "1                BaggingClassifier  0.892892   0.745251  0.679563  0.704936   \n",
       "9                       GaussianNB  0.866239   0.687214  0.707727  0.696568   \n",
       "20                   MultinomialNB  0.872162   0.693752  0.693052  0.693401   \n",
       "19                   MLPClassifier  0.900790   0.790903  0.651582  0.691288   \n",
       "4                     ComplementNB  0.855874   0.674136  0.712668  0.689998   \n",
       "24   QuadraticDiscriminantAnalysis  0.861797   0.677888  0.697994  0.687014   \n",
       "2                      BernoulliNB  0.814413   0.658937  0.777517  0.684522   \n",
       "28                   SGDClassifier  0.890918   0.740760  0.653196  0.682853   \n",
       "3           CalibratedClassifierCV  0.898322   0.780082  0.644772  0.682806   \n",
       "18            LogisticRegressionCV  0.896841   0.771005  0.645735  0.682214   \n",
       "7              ExtraTreeClassifier  0.858342   0.670175  0.688820  0.678653   \n",
       "16                       LinearSVC  0.790721   0.658556  0.812771  0.678205   \n",
       "17              LogisticRegression  0.895854   0.768226  0.639765  0.675930   \n",
       "21                 NearestCentroid  0.794176   0.654741  0.796697  0.675566   \n",
       "5           DecisionTreeClassifier  0.853899   0.659650  0.675481  0.666901   \n",
       "26                 RidgeClassifier  0.897828   0.791270  0.624655  0.663200   \n",
       "27               RidgeClassifierCV  0.897335   0.789299  0.622571  0.660651   \n",
       "10       GaussianProcessClassifier  0.862290   0.660018  0.640566  0.649223   \n",
       "23                      Perceptron  0.880059   0.694345  0.594739  0.619051   \n",
       "29                             SVC  0.891905   0.780540  0.583424  0.611760   \n",
       "22     PassiveAggressiveClassifier  0.885489   0.757335  0.538307  0.542842   \n",
       "13                LabelPropagation  0.848963   0.519835  0.508574  0.503470   \n",
       "14                  LabelSpreading  0.848963   0.519835  0.508574  0.503470   \n",
       "6                  DummyClassifier  0.881540   0.440770  0.500000  0.468520   \n",
       "\n",
       "         AUC  Balanced Accuracy  \n",
       "12  0.705403           0.705403  \n",
       "0   0.703600           0.703600  \n",
       "25  0.689609           0.689609  \n",
       "11  0.694023           0.694023  \n",
       "15  0.680403           0.680403  \n",
       "8   0.673346           0.673346  \n",
       "1   0.679563           0.679563  \n",
       "9   0.707727           0.707727  \n",
       "20  0.693052           0.693052  \n",
       "19  0.651582           0.651582  \n",
       "4   0.712668           0.712668  \n",
       "24  0.697994           0.697994  \n",
       "2   0.777517           0.777517  \n",
       "28  0.653196           0.653196  \n",
       "3   0.644772           0.644772  \n",
       "18  0.645735           0.645735  \n",
       "7   0.688820           0.688820  \n",
       "16  0.812771           0.812771  \n",
       "17  0.639765           0.639765  \n",
       "21  0.796697           0.796697  \n",
       "5   0.675481           0.675481  \n",
       "26  0.624655           0.624655  \n",
       "27  0.622571           0.622571  \n",
       "10  0.640566           0.640566  \n",
       "23  0.594739           0.594739  \n",
       "29  0.583424           0.583424  \n",
       "22  0.538307           0.538307  \n",
       "13  0.508574           0.508574  \n",
       "14  0.508574           0.508574  \n",
       "6   0.500000           0.500000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"../results/all_cities_random_shuffle.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: Comparing Model Performance on Big and Small Cities\n",
    "\n",
    "This experiment involves splitting the dataset based on two sets of cities:\n",
    "1. **Big Cities**: Berlin, Munich, Stuttgart, Frankfurt.\n",
    "2. **Small Cities**: Karlsruhe, Trier, Saarbrücken, Mainz.\n",
    "\n",
    "For each group (big and small cities), models are trained by excluding one city as the test set and using the remaining cities as the training set. This process is repeated for all possible combinations of cities in each group, and the performance of the models is recorded.\n",
    "\n",
    "#### Steps:\n",
    "1. **City Splitting**:\n",
    "   - For each city in the `big_cities` list, the dataset is split such that one city is used as the test set, and the rest are used for training. The same is done for `small_cities`.\n",
    "\n",
    "2. **Data Splitting**:\n",
    "   - The `data_splitter` function is used to create the train and test splits for each combination of cities.\n",
    "\n",
    "3. **Model Training and Evaluation**:\n",
    "   - The `run_experiment` function is used to train multiple classifiers on the training data and evaluate them on the test data.\n",
    "   - The metrics collected include accuracy, precision, recall, F1-score, AUC, and balanced accuracy.\n",
    "\n",
    "4. **Saving Results**:\n",
    "   - The results for each city test case are saved in a CSV file for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19e525370054473491131df770ea025d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2e07ed74d8a49088b29671a893d8996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5a741cd2e3447c1852a2da9909f2fcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b4f9b10433042d6a53a18dcd72ed71b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "209d3c45389a4c398d919127648a87e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5205a0c8cfe54d72b7ef3a00bbbf25a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e8fb2b74a144e786ab314ae26f7f67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36d253d0fbe5438fae0cc25fb900915e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f22b445af6e4af99593ab6f058f137b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eff8720e9c84083ae3c5b043b27f897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Berlin, Munich, Stuttgart, Frankfurt: Big CITY EXP-1\n",
    "Kalsruhe, trier, saarbrucken, mainz: EXP-2\n",
    "\"\"\"\n",
    "\n",
    "# EXP-1\n",
    "big_cities = ['Berlin', 'Munich', 'Stuttgart', 'Frankfurt']\n",
    "small_cities = ['Karlsruhe', 'Trier', 'Saarbrücken', 'Mainz']\n",
    "\n",
    "\n",
    "# make a table in the end to summarise the results of all experiments\n",
    "\n",
    "# big cities splited in trian and test where only one big city is test and all possible combinations for this\n",
    "for city in tqdm(big_cities):\n",
    "    test_cities = [city]\n",
    "    train_cities = [x for x in big_cities if x != city]\n",
    "    X_train, X_test, y_train, y_test = data_splitter(data, train_cities=train_cities, test_cities=test_cities)\n",
    "    results_df, models = run_experiment(X_train, X_test, y_train, y_test)\n",
    "    results_df.to_csv(f\"../results/big_cities_test_city_{city}_.csv\", index=False)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# small cities splited in trian and test where only one small city is test and all possible combinations for this\n",
    "for city in tqdm(small_cities):\n",
    "    test_cities = [city]\n",
    "    train_cities = [x for x in small_cities if x != city]\n",
    "    X_train, X_test, y_train, y_test = data_splitter(data, train_cities=train_cities, test_cities=test_cities)\n",
    "    results_df, models = run_experiment(X_train, X_test, y_train, y_test)\n",
    "    results_df.to_csv(f\"../results/small_cities_test_city_{city}_.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.847561</td>\n",
       "      <td>0.633567</td>\n",
       "      <td>0.812410</td>\n",
       "      <td>0.665443</td>\n",
       "      <td>0.812410</td>\n",
       "      <td>0.812410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>SGDClassifier</td>\n",
       "      <td>0.941057</td>\n",
       "      <td>0.871992</td>\n",
       "      <td>0.612098</td>\n",
       "      <td>0.662336</td>\n",
       "      <td>0.612098</td>\n",
       "      <td>0.612098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BaggingClassifier</td>\n",
       "      <td>0.930894</td>\n",
       "      <td>0.737337</td>\n",
       "      <td>0.619819</td>\n",
       "      <td>0.654837</td>\n",
       "      <td>0.619819</td>\n",
       "      <td>0.619819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>0.941057</td>\n",
       "      <td>0.908574</td>\n",
       "      <td>0.598906</td>\n",
       "      <td>0.647382</td>\n",
       "      <td>0.598906</td>\n",
       "      <td>0.598906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.894309</td>\n",
       "      <td>0.626664</td>\n",
       "      <td>0.652892</td>\n",
       "      <td>0.637969</td>\n",
       "      <td>0.652892</td>\n",
       "      <td>0.652892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.912602</td>\n",
       "      <td>0.651632</td>\n",
       "      <td>0.623163</td>\n",
       "      <td>0.635386</td>\n",
       "      <td>0.623163</td>\n",
       "      <td>0.623163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ExtraTreeClassifier</td>\n",
       "      <td>0.904472</td>\n",
       "      <td>0.635564</td>\n",
       "      <td>0.631979</td>\n",
       "      <td>0.633737</td>\n",
       "      <td>0.631979</td>\n",
       "      <td>0.631979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>PassiveAggressiveClassifier</td>\n",
       "      <td>0.941057</td>\n",
       "      <td>0.970165</td>\n",
       "      <td>0.585714</td>\n",
       "      <td>0.630965</td>\n",
       "      <td>0.585714</td>\n",
       "      <td>0.585714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ComplementNB</td>\n",
       "      <td>0.880081</td>\n",
       "      <td>0.605113</td>\n",
       "      <td>0.645233</td>\n",
       "      <td>0.620127</td>\n",
       "      <td>0.645233</td>\n",
       "      <td>0.645233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>0.922764</td>\n",
       "      <td>0.676409</td>\n",
       "      <td>0.589059</td>\n",
       "      <td>0.614229</td>\n",
       "      <td>0.589059</td>\n",
       "      <td>0.589059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.645339</td>\n",
       "      <td>0.585777</td>\n",
       "      <td>0.605206</td>\n",
       "      <td>0.585777</td>\n",
       "      <td>0.585777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.932927</td>\n",
       "      <td>0.781508</td>\n",
       "      <td>0.568146</td>\n",
       "      <td>0.598745</td>\n",
       "      <td>0.568146</td>\n",
       "      <td>0.568146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>0.920732</td>\n",
       "      <td>0.657038</td>\n",
       "      <td>0.574773</td>\n",
       "      <td>0.596747</td>\n",
       "      <td>0.574773</td>\n",
       "      <td>0.574773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>RidgeClassifier</td>\n",
       "      <td>0.930894</td>\n",
       "      <td>0.746722</td>\n",
       "      <td>0.567052</td>\n",
       "      <td>0.595551</td>\n",
       "      <td>0.567052</td>\n",
       "      <td>0.567052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>RidgeClassifierCV</td>\n",
       "      <td>0.930894</td>\n",
       "      <td>0.746722</td>\n",
       "      <td>0.567052</td>\n",
       "      <td>0.595551</td>\n",
       "      <td>0.567052</td>\n",
       "      <td>0.567052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NearestCentroid</td>\n",
       "      <td>0.760163</td>\n",
       "      <td>0.597732</td>\n",
       "      <td>0.804939</td>\n",
       "      <td>0.595338</td>\n",
       "      <td>0.804939</td>\n",
       "      <td>0.804939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegressionCV</td>\n",
       "      <td>0.914634</td>\n",
       "      <td>0.627239</td>\n",
       "      <td>0.571491</td>\n",
       "      <td>0.588530</td>\n",
       "      <td>0.571491</td>\n",
       "      <td>0.571491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GaussianProcessClassifier</td>\n",
       "      <td>0.908537</td>\n",
       "      <td>0.605513</td>\n",
       "      <td>0.568209</td>\n",
       "      <td>0.580991</td>\n",
       "      <td>0.568209</td>\n",
       "      <td>0.568209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LinearDiscriminantAnalysis</td>\n",
       "      <td>0.908537</td>\n",
       "      <td>0.605513</td>\n",
       "      <td>0.568209</td>\n",
       "      <td>0.580991</td>\n",
       "      <td>0.568209</td>\n",
       "      <td>0.568209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>0.932927</td>\n",
       "      <td>0.801440</td>\n",
       "      <td>0.554955</td>\n",
       "      <td>0.580064</td>\n",
       "      <td>0.554955</td>\n",
       "      <td>0.554955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>0.922764</td>\n",
       "      <td>0.649594</td>\n",
       "      <td>0.549484</td>\n",
       "      <td>0.566701</td>\n",
       "      <td>0.549484</td>\n",
       "      <td>0.549484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CalibratedClassifierCV</td>\n",
       "      <td>0.934959</td>\n",
       "      <td>0.967280</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>0.562034</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>0.542857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>0.918699</td>\n",
       "      <td>0.621487</td>\n",
       "      <td>0.547296</td>\n",
       "      <td>0.561966</td>\n",
       "      <td>0.547296</td>\n",
       "      <td>0.547296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>HistGradientBoostingClassifier</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.681296</td>\n",
       "      <td>0.538481</td>\n",
       "      <td>0.552320</td>\n",
       "      <td>0.538481</td>\n",
       "      <td>0.538481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.631902</td>\n",
       "      <td>0.512098</td>\n",
       "      <td>0.507288</td>\n",
       "      <td>0.512098</td>\n",
       "      <td>0.512098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DummyClassifier</td>\n",
       "      <td>0.928862</td>\n",
       "      <td>0.464431</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.481560</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>0.928862</td>\n",
       "      <td>0.464431</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.481560</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.928862</td>\n",
       "      <td>0.464431</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.481560</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LabelPropagation</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.464358</td>\n",
       "      <td>0.498906</td>\n",
       "      <td>0.481013</td>\n",
       "      <td>0.498906</td>\n",
       "      <td>0.498906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LabelSpreading</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.464358</td>\n",
       "      <td>0.498906</td>\n",
       "      <td>0.481013</td>\n",
       "      <td>0.498906</td>\n",
       "      <td>0.498906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Model  Accuracy  Precision    Recall  F1-score   \n",
       "2                      BernoulliNB  0.847561   0.633567  0.812410  0.665443  \\\n",
       "28                   SGDClassifier  0.941057   0.871992  0.612098  0.662336   \n",
       "1                BaggingClassifier  0.930894   0.737337  0.619819  0.654837   \n",
       "8             ExtraTreesClassifier  0.941057   0.908574  0.598906  0.647382   \n",
       "20                   MultinomialNB  0.894309   0.626664  0.652892  0.637969   \n",
       "17              LogisticRegression  0.912602   0.651632  0.623163  0.635386   \n",
       "7              ExtraTreeClassifier  0.904472   0.635564  0.631979  0.633737   \n",
       "22     PassiveAggressiveClassifier  0.941057   0.970165  0.585714  0.630965   \n",
       "4                     ComplementNB  0.880081   0.605113  0.645233  0.620127   \n",
       "0               AdaBoostClassifier  0.922764   0.676409  0.589059  0.614229   \n",
       "5           DecisionTreeClassifier  0.916667   0.645339  0.585777  0.605206   \n",
       "25          RandomForestClassifier  0.932927   0.781508  0.568146  0.598745   \n",
       "11      GradientBoostingClassifier  0.920732   0.657038  0.574773  0.596747   \n",
       "26                 RidgeClassifier  0.930894   0.746722  0.567052  0.595551   \n",
       "27               RidgeClassifierCV  0.930894   0.746722  0.567052  0.595551   \n",
       "21                 NearestCentroid  0.760163   0.597732  0.804939  0.595338   \n",
       "18            LogisticRegressionCV  0.914634   0.627239  0.571491  0.588530   \n",
       "10       GaussianProcessClassifier  0.908537   0.605513  0.568209  0.580991   \n",
       "15      LinearDiscriminantAnalysis  0.908537   0.605513  0.568209  0.580991   \n",
       "23                      Perceptron  0.932927   0.801440  0.554955  0.580064   \n",
       "24   QuadraticDiscriminantAnalysis  0.922764   0.649594  0.549484  0.566701   \n",
       "3           CalibratedClassifierCV  0.934959   0.967280  0.542857  0.562034   \n",
       "16                       LinearSVC  0.918699   0.621487  0.547296  0.561966   \n",
       "12  HistGradientBoostingClassifier  0.926829   0.681296  0.538481  0.552320   \n",
       "19                   MLPClassifier  0.926829   0.631902  0.512098  0.507288   \n",
       "6                  DummyClassifier  0.928862   0.464431  0.500000  0.481560   \n",
       "9                       GaussianNB  0.928862   0.464431  0.500000  0.481560   \n",
       "29                             SVC  0.928862   0.464431  0.500000  0.481560   \n",
       "13                LabelPropagation  0.926829   0.464358  0.498906  0.481013   \n",
       "14                  LabelSpreading  0.926829   0.464358  0.498906  0.481013   \n",
       "\n",
       "         AUC  Balanced Accuracy  \n",
       "2   0.812410           0.812410  \n",
       "28  0.612098           0.612098  \n",
       "1   0.619819           0.619819  \n",
       "8   0.598906           0.598906  \n",
       "20  0.652892           0.652892  \n",
       "17  0.623163           0.623163  \n",
       "7   0.631979           0.631979  \n",
       "22  0.585714           0.585714  \n",
       "4   0.645233           0.645233  \n",
       "0   0.589059           0.589059  \n",
       "5   0.585777           0.585777  \n",
       "25  0.568146           0.568146  \n",
       "11  0.574773           0.574773  \n",
       "26  0.567052           0.567052  \n",
       "27  0.567052           0.567052  \n",
       "21  0.804939           0.804939  \n",
       "18  0.571491           0.571491  \n",
       "10  0.568209           0.568209  \n",
       "15  0.568209           0.568209  \n",
       "23  0.554955           0.554955  \n",
       "24  0.549484           0.549484  \n",
       "3   0.542857           0.542857  \n",
       "16  0.547296           0.547296  \n",
       "12  0.538481           0.538481  \n",
       "19  0.512098           0.512098  \n",
       "6   0.500000           0.500000  \n",
       "9   0.500000           0.500000  \n",
       "29  0.500000           0.500000  \n",
       "13  0.498906           0.498906  \n",
       "14  0.498906           0.498906  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating AUC Scores and Identifying the Best Models\n",
    "\n",
    "This script aggregates the AUC (Area Under the Curve) scores for all models across multiple experiments, calculates the average AUC for each model, and selects the top-performing models based on their average AUC. The goal is to identify the models that consistently perform well across different test cities.\n",
    "\n",
    "#### Steps:\n",
    "1. **Load Experiment Results**:\n",
    "   - The script reads all CSV files from the `../results/` directory, where each file contains the results of a specific experiment.\n",
    "\n",
    "2. **Sum AUC Scores**:\n",
    "   - For each model in the results, the script sums up the AUC scores across different experiments and also counts the number of times each model appears.\n",
    "\n",
    "3. **Calculate Average AUC**:\n",
    "   - The average AUC for each model is computed by dividing the total AUC by the number of times the model was evaluated.\n",
    "\n",
    "4. **Sort and Select Top Models**:\n",
    "   - Models are then sorted based on their average AUC in descending order, and the top 5 models are selected for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../results\\all_cities_random_shuffle.csv\n",
      "../results\\big_cities_test_city_Berlin_.csv\n",
      "../results\\big_cities_test_city_Frankfurt_.csv\n",
      "../results\\big_cities_test_city_Munich_.csv\n",
      "../results\\big_cities_test_city_Stuttgart_.csv\n",
      "../results\\small_cities_test_city_Karlsruhe_.csv\n",
      "../results\\small_cities_test_city_Mainz_.csv\n",
      "../results\\small_cities_test_city_Saarbrücken_.csv\n",
      "../results\\small_cities_test_city_Trier_.csv\n",
      "              Model  Average AUC\n",
      "12      BernoulliNB     0.779203\n",
      "16  NearestCentroid     0.766557\n",
      "10     ComplementNB     0.671147\n",
      "9     MultinomialNB     0.662987\n",
      "25    SGDClassifier     0.658037\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Get a list of all result files from different experiments\n",
    "result_files = glob.glob(\"../results/*.csv\")\n",
    "\n",
    "# Create a dictionary to store the total AUC and count for each model\n",
    "auc_sum_per_model = {}\n",
    "count_per_model = {}\n",
    "\n",
    "# Iterate over each result file\n",
    "for file in result_files:\n",
    "    print(file)\n",
    "    # Load the results for each experiment\n",
    "    results = pd.read_csv(file)\n",
    "    \n",
    "    # Iterate over each row in the results\n",
    "    for _, row in results.iterrows():\n",
    "        model = row['Model']\n",
    "        auc = row['AUC']\n",
    "        \n",
    "        # Update the total AUC and count for the model\n",
    "        if model in auc_sum_per_model:\n",
    "            auc_sum_per_model[model] += auc\n",
    "            count_per_model[model] += 1\n",
    "        else:\n",
    "            auc_sum_per_model[model] = auc\n",
    "            count_per_model[model] = 1\n",
    "\n",
    "# Calculate the average AUC for each model\n",
    "average_auc_per_model = {model: auc_sum_per_model[model] / count_per_model[model] for model in auc_sum_per_model}\n",
    "\n",
    "# Create a DataFrame from the average AUC dictionary\n",
    "average_auc_df = pd.DataFrame(list(average_auc_per_model.items()), columns=['Model', 'Average AUC'])\n",
    "\n",
    "# Sort the DataFrame by Average AUC in descending order\n",
    "sorted_models = average_auc_df.sort_values(by='Average AUC', ascending=False)\n",
    "\n",
    "# Select the top 5 models\n",
    "top_5_models = sorted_models.head(5)\n",
    "\n",
    "# Display the best models\n",
    "print(top_5_models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Average AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.779203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NearestCentroid</td>\n",
       "      <td>0.766557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ComplementNB</td>\n",
       "      <td>0.671147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.662987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>SGDClassifier</td>\n",
       "      <td>0.658037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Model  Average AUC\n",
       "12      BernoulliNB     0.779203\n",
       "16  NearestCentroid     0.766557\n",
       "10     ComplementNB     0.671147\n",
       "9     MultinomialNB     0.662987\n",
       "25    SGDClassifier     0.658037"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_5_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining and Summarizing Experiment Results by City Type\n",
    "\n",
    "This script processes the results from various classification experiments, categorizes them by city type (big, small, and all), calculates the average AUC (Area Under the Curve) for the models used in these experiments, and generates a summary DataFrame displaying these results.\n",
    "\n",
    "#### Steps:\n",
    "1. **Load Experiment Results**:\n",
    "   - The script reads all CSV files containing the results of classification experiments from the `../results/` directory.\n",
    "\n",
    "2. **Group Results by City Type**:\n",
    "   - It categorizes the results based on city types: `big`, `small`, and `all`.\n",
    "   - For each city type, it combines the results from the relevant files into a single DataFrame.\n",
    "\n",
    "3. **Calculate Average AUC**:\n",
    "   - The script computes the average AUC for each model within each city type.\n",
    "\n",
    "4. **Select Top Models**:\n",
    "   - It identifies the top 5 models based on their average AUC and filters the combined results accordingly.\n",
    "\n",
    "5. **Generate Summary DataFrame**:\n",
    "   - For each city type, a summary row is created containing the average AUC values for the top models.\n",
    "   - These summary rows are concatenated into a final DataFrame, which is printed at the end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  type_city  BernoulliNB  ComplementNB  GaussianNB  NearestCentroid   \n",
      "0       big     0.764115      0.669090    0.660482         0.715620  \\\n",
      "1     small     0.810223      0.654498         NaN         0.853362   \n",
      "2       all     0.799321      0.673888         NaN         0.834474   \n",
      "\n",
      "   QuadraticDiscriminantAnalysis  LinearSVC  MultinomialNB  SGDClassifier  \n",
      "0                       0.657869        NaN            NaN            NaN  \n",
      "1                            NaN   0.685329       0.666221            NaN  \n",
      "2                            NaN        NaN       0.675165       0.676748  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Get a list of all result files from different experiments\n",
    "result_files = glob.glob(\"../results/*.csv\")\n",
    "\n",
    "# Create an empty DataFrame to store the combined results\n",
    "combined_results = pd.DataFrame()\n",
    "\n",
    "# Create an empty DataFrame to store the summary\n",
    "summary_results = pd.DataFrame(columns=['type_city'])\n",
    "\n",
    "# Iterate over each result file\n",
    "for type_city in ['big', 'small', 'all']:\n",
    "    # Reset combined_results for each type_city iteration\n",
    "    combined_results = pd.DataFrame()\n",
    "\n",
    "    # List to store dataframes to be concatenated later\n",
    "    data_frames = []\n",
    "\n",
    "    # Iterate over each result file\n",
    "    for file in result_files:\n",
    "        # Load the results for each experiment\n",
    "        if type_city in file:\n",
    "            results = pd.read_csv(file)\n",
    "            \n",
    "            # Append the results to the list of dataframes\n",
    "            data_frames.append(results)\n",
    "\n",
    "    # Concatenate all dataframes in the list\n",
    "    if data_frames:\n",
    "        combined_results = pd.concat(data_frames)\n",
    "\n",
    "    # Calculate the average AUC for each model\n",
    "    average_auc_per_model = combined_results.groupby('Model')['AUC'].mean()\n",
    "    \n",
    "    # Sort the models by average AUC in descending order\n",
    "    sorted_models = average_auc_per_model.sort_values(ascending=False)\n",
    "    \n",
    "    # Get the top 5 models\n",
    "    top_5_models = sorted_models.head(5).index.tolist()\n",
    "\n",
    "    # Filter the results to include only the rows corresponding to the top 5 models\n",
    "    filtered_results = combined_results[combined_results['Model'].isin(top_5_models)]\n",
    "\n",
    "    # Calculate the average AUC for each model\n",
    "    average_auc_by_model = filtered_results.groupby('Model')['AUC'].mean()\n",
    "    \n",
    "    # Create a row with type_city and average AUC values for each model\n",
    "    row = {'type_city': type_city}\n",
    "    row.update(average_auc_by_model.to_dict())\n",
    "    \n",
    "    # Append the row to the summary_results DataFrame\n",
    "    summary_results = pd.concat([summary_results, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "# Display the summary_results DataFrame\n",
    "print(summary_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type_city</th>\n",
       "      <th>BernoulliNB</th>\n",
       "      <th>ComplementNB</th>\n",
       "      <th>GaussianNB</th>\n",
       "      <th>NearestCentroid</th>\n",
       "      <th>QuadraticDiscriminantAnalysis</th>\n",
       "      <th>LinearSVC</th>\n",
       "      <th>MultinomialNB</th>\n",
       "      <th>SGDClassifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>big</td>\n",
       "      <td>0.764115</td>\n",
       "      <td>0.669090</td>\n",
       "      <td>0.660482</td>\n",
       "      <td>0.715620</td>\n",
       "      <td>0.657869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>small</td>\n",
       "      <td>0.810223</td>\n",
       "      <td>0.654498</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.853362</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.685329</td>\n",
       "      <td>0.666221</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>all</td>\n",
       "      <td>0.799321</td>\n",
       "      <td>0.673888</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.834474</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.675165</td>\n",
       "      <td>0.676748</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  type_city  BernoulliNB  ComplementNB  GaussianNB  NearestCentroid   \n",
       "0       big     0.764115      0.669090    0.660482         0.715620  \\\n",
       "1     small     0.810223      0.654498         NaN         0.853362   \n",
       "2       all     0.799321      0.673888         NaN         0.834474   \n",
       "\n",
       "   QuadraticDiscriminantAnalysis  LinearSVC  MultinomialNB  SGDClassifier  \n",
       "0                       0.657869        NaN            NaN            NaN  \n",
       "1                            NaN   0.685329       0.666221            NaN  \n",
       "2                            NaN        NaN       0.675165       0.676748  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing Experiment Metrics by City Type\n",
    "\n",
    "This script analyzes the results from various classification experiments, specifically focusing on the metrics for the top-performing models based on city types (big, small, and all). It calculates the average values for AUC (Area Under the Curve), Accuracy, Precision, and Recall, generating a concise summary of model performance.\n",
    "\n",
    "#### Steps:\n",
    "1. **Load Experiment Results**:\n",
    "   - The script retrieves all CSV files containing experiment results from the `../results/` directory.\n",
    "\n",
    "2. **Group Results by City Type**:\n",
    "   - It categorizes results based on city types: `big`, `small`, and `all`.\n",
    "   - For each city type, it concatenates the results from relevant files into a single DataFrame.\n",
    "\n",
    "3. **Filter for Top Models**:\n",
    "   - It filters the combined results to retain only those corresponding to the top 5 models (assumed to be pre-defined in the variable `top_5_models`).\n",
    "\n",
    "4. **Calculate Average Metrics**:\n",
    "   - The average values for AUC, Accuracy, Precision, and Recall are computed for the top 5 models within each city type.\n",
    "\n",
    "5. **Generate Summary DataFrame**:\n",
    "   - For each city type, a summary row is created containing the average metrics, which are appended to a final summary DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  type_city       AUC  Accuracy  Precision    Recall\n",
      "0       big  0.689336  0.819802   0.662562  0.689336\n",
      "1     small  0.722180  0.887707   0.654480  0.722180\n",
      "2       all  0.731919  0.860643   0.656496  0.731919\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Get a list of all result files from different experiments\n",
    "result_files = glob.glob(\"../results/*.csv\")\n",
    "\n",
    "# Create an empty DataFrame to store the summary\n",
    "summary_results = pd.DataFrame(columns=['type_city', 'AUC', 'Accuracy', 'Precision', 'Recall'])\n",
    "\n",
    "# Iterate over each type of city\n",
    "for type_city in ['big', 'small', 'all']:\n",
    "    # List to store the DataFrames for each file of the current type_city\n",
    "    data_frames = []\n",
    "\n",
    "    # Iterate over each result file\n",
    "    for file in result_files:\n",
    "        # Load the results for each experiment\n",
    "        if type_city in file:\n",
    "            results = pd.read_csv(file)\n",
    "            # Add to the list of dataframes to be concatenated\n",
    "            data_frames.append(results)\n",
    "\n",
    "    # Concatenate all dataframes into one DataFrame if data exists\n",
    "    if data_frames:\n",
    "        combined_results = pd.concat(data_frames)\n",
    "\n",
    "        # Filter the results to include only the rows corresponding to the top 5 models\n",
    "        filtered_results = combined_results[combined_results['Model'].isin(top_5_models)]\n",
    "\n",
    "        # Calculate the average values for each metric (AUC, Accuracy, Precision, Recall)\n",
    "        average_metrics_per_model = filtered_results.groupby('Model')[['AUC', 'Accuracy', 'Precision', 'Recall']].mean()\n",
    "\n",
    "        # Calculate the mean of the average metrics across the top 5 models\n",
    "        average_values = average_metrics_per_model.mean()\n",
    "\n",
    "        # Create a row with type_city and the average metric values\n",
    "        row = {'type_city': type_city}\n",
    "        for metric in ['AUC', 'Accuracy', 'Precision', 'Recall']:\n",
    "            row[metric] = average_values[metric]\n",
    "\n",
    "        # Append the row to the summary_results DataFrame\n",
    "        summary_results = pd.concat([summary_results, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "# Display the summary_results DataFrame\n",
    "print(summary_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NearestCentroid',\n",
       " 'BernoulliNB',\n",
       " 'SGDClassifier',\n",
       " 'MultinomialNB',\n",
       " 'ComplementNB']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_5_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_results.to_csv(\"../results/summary_results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
