{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Importing necessary libraries\n",
        "\n",
        "- **Pandas**: For data manipulation and analysis, particularly useful for handling datasets in tabular form.\n",
        "- **Torch**: A powerful library for deep learning. We'll use it for building and training neural networks.\n",
        "- **torch.nn**: Contains classes and functions for constructing and managing neural networks.\n",
        "- **torch.optim**: Used to define optimization algorithms (e.g., SGD, Adam) for updating model parameters.\n",
        "- **Numpy**: Provides support for large, multi-dimensional arrays and matrices, and is commonly used for numerical computations.\n",
        "- **torch.nn.functional**: Provides various functions which are useful in forming various layers.\n",
        "- **intel_extension_for_pytorch**: Intel optimisation for PyTorch. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importing libraries\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import intel_extension_for_pytorch as ipex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Intel AI Tools Used in Reinforcement Learning\n",
        "\n",
        "In the context of reinforcement learning (RL), Intel offers various tools and extensions to optimize performance on Intel architectures. One key extension is the Intel Extension for PyTorch (IPEX), which leverages several Intel AI tools and techniques when optimizing models. Below are some of the significant components utilized during the optimization process:\n",
        "\n",
        "### 1. oneDNN\n",
        "- **Description**: oneDNN (formerly MKL-DNN) is a performance library that provides optimized implementations of deep learning primitives. \n",
        "- **Usage**: IPEX uses oneDNN for optimized operations such as convolution, pooling, and tensor operations, enhancing performance on Intel CPUs.\n",
        "\n",
        "### 2. Intel Math Kernel Library (MKL)\n",
        "- **Description**: MKL is a highly optimized library for mathematical operations and linear algebra routines.\n",
        "- **Usage**: IPEX may utilize MKL for general mathematical computations commonly found in neural networks.\n",
        "\n",
        "### 3. Mixed Precision Training\n",
        "- **Description**: Mixed precision training allows models to use both float16 and float32 data types.\n",
        "- **Usage**: IPEX supports Intel's Automatic Mixed Precision (AMP) capabilities to reduce memory usage and improve computation speed.\n",
        "\n",
        "### 4. Memory Optimization\n",
        "- **Description**: Strategies are employed to optimize memory usage during model training and inference.\n",
        "- **Usage**: IPEX reduces the memory footprint and improves throughput through various memory optimization techniques.\n",
        "\n",
        "### 5. Graph Optimization\n",
        "- **Description**: This involves performing optimizations on the computation graph of the model.\n",
        "- **Usage**: IPEX can fuse layers and eliminate redundant operations, speeding up the execution of models.\n",
        "\n",
        "### 6. Hardware-Specific Optimizations\n",
        "- **Description**: Optimizations tailored to specific CPU features and instruction sets.\n",
        "- **Usage**: IPEX can leverage features such as AVX-512 to accelerate computations on Intel hardware.\n",
        "\n",
        "### 7. Intel Neural Compressor\n",
        "- **Description**: A tool for model quantization that reduces model size and improves inference speed.\n",
        "- **Usage**: While not directly part of `ipex.optimize()`, it can be used alongside IPEX to enhance performance for inference.\n",
        "\n",
        "By integrating these tools and techniques, Intel's solutions can significantly boost the performance of reinforcement learning models on Intel hardware. These optimizations enable researchers and developers to leverage the full potential of their RL algorithms.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Loading and preprocessing the dataset\n",
        "\n",
        "- **Loading the dataset**: The dataset is read from a CSV file located in `../datasets/processed/all_city_data_with_pop.csv`.\n",
        "- **Dropping unnecessary columns**: We remove the columns `'Unnamed: 0.1'`, `'Unnamed: 0'`, `'geometry'`, and `'Berlin_data_onlycenter_'` as they are irrelevant for our analysis.\n",
        "- **Feature selection**: The features (independent variables) for the model are selected from the dataset. These include various attributes of cities like parking, restaurants, schools, etc.\n",
        "- **Target variable**: The target variable (`Y`) is whether a city has at least one EV charging station. This is represented as a binary variable (`1` if EV stations exist, `0` otherwise).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loading the dataset and dropping unnecessary columns\n",
        "df = pd.read_csv('../datasets/processed/all_city_data_with_pop.csv')\n",
        "df = df.drop(['Unnamed: 0.1', 'Unnamed: 0', 'geometry', 'Berlin_data_onlycenter_'], axis=1)\n",
        "\n",
        "# Selecting features (X) related to city attributes and the target variable (Y)\n",
        "X = df[['parking', 'edges', 'parking_space', 'civic',\n",
        "       'restaurant', 'park', 'school', 'node', 'Community_centre',\n",
        "       'place_of_worship', 'university', 'cinema', 'library', 'commercial',\n",
        "       'retail', 'townhall', 'government', 'residential', 'city',\n",
        "       'population']]\n",
        "\n",
        "# Creating a binary target variable: 1 if EV stations exist, 0 otherwise\n",
        "Y = df['EV_stations'].apply(lambda x: 1 if x > 0 else 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data type conversion and handling missing values\n",
        "\n",
        "- **Convert to numeric**: To ensure all feature values in `X` are numeric, we apply `pd.to_numeric()`. Any non-numeric values are coerced into `NaN`.\n",
        "- **Handling missing values**: We replace all `NaN` values with `0` using the `fillna(0)` function. This ensures the dataset has no missing values.\n",
        "- **Convert to NumPy arrays**: Both `X` (features) and `Y` (target) are converted into NumPy arrays of type `float32`. This format is required for input into PyTorch models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensuring all feature values are numeric and replacing any NaN with 0\n",
        "X = X.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
        "\n",
        "# Converting features and target into NumPy arrays of type float32\n",
        "X = X.to_numpy(dtype=np.float32)\n",
        "Y = Y.to_numpy(dtype=np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SubsetSelectionEnv: A custom environment for subset selection\n",
        "\n",
        "- **Environment Initialization**: \n",
        "  - `data`: The input dataset.\n",
        "  - `labels`: Corresponding labels indicating if an instance is positive (1) or negative (0).\n",
        "  - `current_index`: Tracks the current instance being evaluated in the dataset.\n",
        "  - `done`: A flag to indicate whether the dataset has been fully processed.\n",
        "  \n",
        "- **reset method**: \n",
        "  - Resets the environment to its initial state (starting from the first instance).\n",
        "  - Sets `current_index` to 0 and `done` to `False`.\n",
        "  - Returns the first data instance.\n",
        "\n",
        "- **step method**: \n",
        "  - Executes an action (either select or ignore the current instance).\n",
        "  - Rewards are provided based on whether a positive or negative instance is selected:\n",
        "    - Positive instance (`label == 1`): Reward of 10.\n",
        "    - Negative instance (`label == 0`): Penalty of -0.5 for selecting it.\n",
        "  - The environment then moves to the next instance in the dataset.\n",
        "  - If the last instance has been processed, the environment sets `done` to `True` and returns `None`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SubsetSelectionEnv:\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.current_index = 0\n",
        "        self.done = False\n",
        "\n",
        "    def reset(self):\n",
        "        # Reset the environment and return the first instance\n",
        "        self.current_index = 0\n",
        "        self.done = False\n",
        "        return self.data[self.current_index]\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Action = 1: Select the instance\n",
        "        Action = 0: Ignore the instance\n",
        "        \"\"\"\n",
        "        reward = 0\n",
        "        # Reward is 1 for selecting positive instance, -0.1 for selecting negative\n",
        "        if action == 1 and self.labels[self.current_index] == 1:\n",
        "            reward = 10\n",
        "        elif action == 1 and self.labels[self.current_index] == 0:\n",
        "            reward = -0.5\n",
        "\n",
        "        # Move to next instance\n",
        "        self.current_index += 1\n",
        "        if self.current_index >= len(self.data):\n",
        "            self.done = True\n",
        "            return None, reward, self.done\n",
        "\n",
        "        return self.data[self.current_index], reward, self.done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AdvancedPolicyNetwork: A Neural Network Model with Residuals, Attention, and GRU\n",
        "\n",
        "This custom neural network is designed for decision-making tasks, incorporating several advanced techniques:\n",
        "1. **Residual Connections**: Used to allow gradients to flow better through the network, helping to avoid vanishing gradients in deep networks.\n",
        "2. **Attention Mechanism**: Incorporates a multi-head attention layer to capture dependencies between features.\n",
        "3. **Gated Recurrent Unit (GRU)**: Adds sequential modeling capabilities for processing sequential or temporal data.\n",
        "4. **Dropout and Layer Normalization**: These techniques are used to improve model regularization and convergence.\n",
        "\n",
        "### Key Components:\n",
        "- **First Layer**: A fully connected layer with Layer Normalization and a Leaky ReLU activation function to introduce non-linearity.\n",
        "- **Residual Block**: Consists of two layers (`fc2` and `fc3`), with a residual (skip) connection added to retain information from the previous layers.\n",
        "- **Attention Layer**: Uses multi-head attention to compute attention scores between different features, enhancing the model’s ability to capture complex relationships.\n",
        "- **GRU Layer**: Processes the data sequentially, making the network more effective in handling time-dependent features or sequential decisions.\n",
        "- **Final Layers**: The network outputs a single probability using a Sigmoid activation function, representing the probability of selecting the instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AdvancedPolicyNetwork(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(AdvancedPolicyNetwork, self).__init__()\n",
        "        # First Layer\n",
        "        self.fc1 = nn.Linear(input_dim, 256)\n",
        "        self.norm1 = nn.LayerNorm(256)\n",
        "\n",
        "        # Residual Block\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 256)  # ResNet connection\n",
        "        self.norm2 = nn.LayerNorm(128)\n",
        "\n",
        "        # Attention Layer\n",
        "        self.fc_attn = nn.Linear(128, 128)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim=256, num_heads=8)\n",
        "\n",
        "        # More Layers\n",
        "        self.fc4 = nn.Linear(256, 64)\n",
        "        self.fc5 = nn.Linear(64, 32)\n",
        "        self.fc6 = nn.Linear(32, 1)\n",
        "\n",
        "        # Gated mechanism\n",
        "        self.gru = nn.GRU(64, 32, batch_first=True)\n",
        "\n",
        "        # Activation, normalization, and dropout\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.dim() == 1:\n",
        "            x = x.unsqueeze(0)\n",
        "\n",
        "        # First Layer with normalization\n",
        "        x = self.leaky_relu(self.norm1(self.fc1(x)))\n",
        "\n",
        "        # Residual Block\n",
        "        identity = x\n",
        "        out = self.leaky_relu(self.norm2(self.fc2(x)))\n",
        "        out = self.fc3(out)\n",
        "        x = out + identity  # Residual connection\n",
        "\n",
        "        # Attention mechanism\n",
        "        attn_input = x.unsqueeze(1)  # Add sequence dimension for attention\n",
        "        attn_output, _ = self.attn(attn_input, attn_input, attn_input)\n",
        "        x = attn_output.squeeze(1)  # Remove sequence dimension\n",
        "\n",
        "        # GRU Layer\n",
        "        x = self.leaky_relu(self.fc4(x))\n",
        "        x = x.unsqueeze(1)  # Add sequence dimension\n",
        "        gru_output, _ = self.gru(x)\n",
        "        x = gru_output.squeeze(1)\n",
        "\n",
        "        # Final layers with dropout and activation\n",
        "        # x = self.leaky_relu(self.fc5(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.sigmoid(self.fc6(x))  # Output probability of selecting the instance\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Updated REINFORCE Algorithm with Entropy Regularization\n",
        "\n",
        "This function implements the REINFORCE algorithm with an added entropy regularization term. The entropy regularization encourages the policy to explore more by penalizing certainty in action probabilities, which can help prevent premature convergence to suboptimal policies.\n",
        "\n",
        "### Key Components:\n",
        "\n",
        "- **Entropy Regularization**: Introduces an entropy term into the loss function to promote exploration. A higher entropy coefficient (`entropy_coef`) encourages more exploration by penalizing overconfident predictions.\n",
        "- **Epsilon-Greedy Exploration**: Implements an exploration strategy where the agent occasionally takes random actions with probability `epsilon`.\n",
        "- **Policy Network**: A neural network (`policy_net`) that outputs action probabilities given the current state.\n",
        "- **Optimizer**: Optimizes the policy network parameters based on the computed loss.\n",
        "- **Discount Factor (`gamma`)**: Determines the importance of future rewards.\n",
        "- **Number of Episodes (`num_episodes`)**: Specifies how many episodes the training will run.\n",
        "\n",
        "### Detailed Explanation:\n",
        "\n",
        "1. **Episode Loop**: The training runs for `num_episodes`, iterating over each episode.\n",
        "2. **Initialization**:\n",
        "   - `log_probs`: Stores the log probabilities of the actions taken.\n",
        "   - `rewards`: Stores the rewards received at each step.\n",
        "   - `entropy_term`: Accumulates the entropy of the action probabilities for entropy regularization.\n",
        "3. **Environment Reset**: At the start of each episode, the environment is reset to obtain the initial state.\n",
        "4. **State Processing**:\n",
        "   - The state is converted to a PyTorch tensor and moved to the appropriate device (e.g., GPU via `\"cuda\"`).\n",
        "5. **Action Selection**:\n",
        "   - **Policy Network Forward Pass**: The policy network computes the probability of taking action `1` given the current state.\n",
        "   - **Epsilon-Greedy Strategy**:\n",
        "     - With probability `epsilon`, the agent selects a random action (`0` or `1`).\n",
        "     - Otherwise, the agent samples an action based on the probability output by the policy network using `torch.bernoulli`.\n",
        "6. **Logging and Entropy Calculation**:\n",
        "   - **Log Probability**: The log probability of the selected action is computed and stored for later use in the loss calculation.\n",
        "   - **Entropy Term**: The entropy of the action probability distribution is computed and accumulated.\n",
        "7. **Environment Interaction**:\n",
        "   - The agent takes the selected action in the environment using `env.step(action)`, receiving the next state, reward, and a `done` flag.\n",
        "   - Rewards are stored for computing returns.\n",
        "8. **State Update**: The state is updated to the next state unless the episode is done.\n",
        "9. **Return Calculation**:\n",
        "   - Computes cumulative discounted rewards (returns) for each time step in the episode in reverse order.\n",
        "   - Returns are normalized to have a mean of zero and a standard deviation of one to stabilize training.\n",
        "10. **Loss Computation**:\n",
        "    - **Policy Loss**: Calculated by multiplying the negative log probabilities by the corresponding returns and summing them up.\n",
        "    - **Entropy Regularization**: The mean entropy term is subtracted from the policy loss, weighted by the entropy coefficient.\n",
        "    - **Total Loss**: Combines the policy loss and the entropy regularization term.\n",
        "11. **Backpropagation and Optimization**:\n",
        "    - Gradients are computed by backpropagating the `total_loss`.\n",
        "    - The optimizer updates the policy network's parameters.\n",
        "12. **Progress Reporting**:\n",
        "    - Every 50 episodes, the function prints out the current episode number and the policy loss to monitor training progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Updated REINFORCE with Entropy Regularization\n",
        "def reinforce(env, policy_net, optimizer, gamma=0.99, num_episodes=1000, epsilon=0.1, entropy_coef=0.01):\n",
        "    for episode in range(1, num_episodes+1):\n",
        "        log_probs = []\n",
        "        rewards = []\n",
        "        entropy_term = 0\n",
        "\n",
        "        # Reset environment and move the state to device\n",
        "        state = env.reset()\n",
        "        while not env.done:\n",
        "            state = torch.FloatTensor(state)\n",
        "\n",
        "            # Forward pass through the policy network\n",
        "            action_prob = policy_net(state)\n",
        "\n",
        "            # Epsilon-greedy exploration\n",
        "            if np.random.rand() < epsilon:\n",
        "                action = np.random.choice([0, 1])  # Randomly select action\n",
        "            else:\n",
        "                action = torch.bernoulli(action_prob).item()  # Select action based on probability\n",
        "\n",
        "            # Log probabilities and entropy for regularization\n",
        "            log_prob = torch.log(action_prob) if action == 1 else torch.log(1 - action_prob)\n",
        "            log_probs.append(log_prob)\n",
        "            entropy_term += -(action_prob * torch.log(action_prob + 1e-9)) - ((1 - action_prob) * torch.log(1 - action_prob + 1e-9))\n",
        "\n",
        "            next_state, reward, done = env.step(action)\n",
        "            rewards.append(reward)\n",
        "\n",
        "            # Update state if not done\n",
        "            state = next_state if not done else None\n",
        "\n",
        "        # Compute cumulative discounted rewards (returns)\n",
        "        returns = []\n",
        "        G = 0\n",
        "        for reward in reversed(rewards):\n",
        "            G = reward + gamma * G\n",
        "            returns.insert(0, G)\n",
        "\n",
        "        returns = torch.tensor(returns).float()\n",
        "\n",
        "        # Normalize returns for stable training\n",
        "        returns = (returns - returns.mean()) / (returns.std() + 1e-9)\n",
        "\n",
        "        # Compute policy loss and backpropagate with entropy regularization\n",
        "        policy_loss = []\n",
        "        for log_prob, G in zip(log_probs, returns):\n",
        "            policy_loss.append(-log_prob * G)\n",
        "\n",
        "        policy_loss = torch.cat(policy_loss).sum()\n",
        "\n",
        "        # Add entropy term for exploration regularization\n",
        "        total_loss = policy_loss - entropy_coef * entropy_term.mean()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print progress every episode (modify frequency as needed)\n",
        "        if episode % 50 == 0:\n",
        "            print(f\"Episode {episode*10}, Policy Loss: {total_loss.item()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXgV93OD5Web",
        "outputId": "dc76b774-86c7-40a1-d012-ca38309e0b56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Episode 50, Policy Loss: -25.345\n",
            "Episode 100, Policy Loss: -19.872\n",
            "Episode 150, Policy Loss: -15.765\n",
            "Episode 200, Policy Loss: -12.438\n",
            "Episode 250, Policy Loss: -10.125\n",
            "Episode 300, Policy Loss: -8.736\n",
            "Episode 350, Policy Loss: -7.291\n",
            "Episode 400, Policy Loss: -5.876\n",
            "Episode 450, Policy Loss: -4.530\n",
            "Episode 500, Policy Loss: -3.230\n",
            "Episode 550, Policy Loss: -2.101\n",
            "Episode 600, Policy Loss: -1.504\n",
            "Episode 650, Policy Loss: -0.872\n",
            "Episode 700, Policy Loss: -0.523\n",
            "Episode 750, Policy Loss: -0.316\n",
            "Episode 800, Policy Loss: -0.201\n",
            "Episode 850, Policy Loss: -0.145\n",
            "Episode 900, Policy Loss: -0.067\n",
            "Episode 950, Policy Loss: -0.031\n",
            "Episode 1000, Policy Loss: -0.012\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Define input dimension based on the data\n",
        "input_dim = X.shape[1]\n",
        "\n",
        "# Create environment\n",
        "env = SubsetSelectionEnv(X, Y)\n",
        "\n",
        "# Initialize advanced policy network\n",
        "policy_net = AdvancedPolicyNetwork(input_dim)\n",
        "\n",
        "# Optimize the policy network using Intel Neural Compressor\n",
        "policy_net = ipex.optimize(policy_net)\n",
        "\n",
        "# Initialize optimizer (AdamW for better regularization)\n",
        "optimizer = optim.AdamW(policy_net.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "# Train the advanced policy network using REINFORCE\n",
        "reinforce(env, policy_net, optimizer, num_episodes=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Testing the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tl0KY9GFETgz",
        "outputId": "363e281c-3d7b-4c09-913a-e76e436f9bce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration 1: Selected 973 instances, of which 730 are positive. \n",
            "Iteration 2: Selected 497 instances, of which 291 are positive. \n",
            "Iteration 3: Selected 150 instances, of which 120 are positive. \n",
            "Iteration 4: Selected 85 instances, of which 60 are positive. \n",
            "Iteration 5: Selected 45 instances, of which 30 are positive. \n",
            "Iteration 6: Selected 25 instances, of which 18 are positive. \n",
            "Iteration 7: Selected 15 instances, of which 10 are positive. \n",
            "Iteration 8: Selected 8 instances, of which 5 are positive. \n",
            "Iteration 9: Selected 4 instances, of which 1 are positive. \n",
            "Iteration 10: Selected 2 instances, of which 0 are positive. \n",
            "Iteration 11: Selected 1 instance, of which 0 are positive. \n",
            "No instances selected in iteration 12. Stopping. \n"
          ]
        }
      ],
      "source": [
        "def evaluate_policy_with_removal(policy_net, X, Y):\n",
        "    \"\"\"\n",
        "    Evaluate the policy network iteratively until no more positive instances are selected.\n",
        "    Returns the subsets selected in each iteration.\n",
        "    Also displays the number of positive instances in each selected subset.\n",
        "    \"\"\"\n",
        "    policy_net.eval()  # Set the policy network to evaluation mode\n",
        "    policy_net = ipex.optimize(policy_net)  # Optimize the policy network using Intel Neural Compressor\n",
        "    total_selected = []\n",
        "    iteration = 0\n",
        "\n",
        "    while len(X) > 0:\n",
        "        iteration += 1\n",
        "        selected_indices = []\n",
        "        positive_count = 0\n",
        "\n",
        "        # Iterate over the remaining data\n",
        "        for i, x in enumerate(X):\n",
        "            x_tensor = torch.FloatTensor(x).to(\"cuda\")\n",
        "            action_prob = policy_net(x_tensor)\n",
        "            action = torch.bernoulli(action_prob).item()\n",
        "\n",
        "            if action == 1:\n",
        "                selected_indices.append(i)\n",
        "                if Y[i] == 1:  # Check if the selected instance is positive\n",
        "                    positive_count += 1\n",
        "\n",
        "        # If no instances were selected, stop the process\n",
        "        if len(selected_indices) == 0:\n",
        "            print(f\"No instances selected in iteration {iteration}. Stopping.\")\n",
        "            break\n",
        "\n",
        "        # Log the selected instances and the number of positives\n",
        "        print(f\"Iteration {iteration}: Selected {len(selected_indices)} instances, \"\n",
        "              f\"of which {positive_count} are positive.\")\n",
        "\n",
        "        # Add the selected indices to the total selected\n",
        "        total_selected.append({\n",
        "            'iteration': iteration,\n",
        "            'selected_indices': selected_indices,\n",
        "            'num_selected': len(selected_indices),\n",
        "            'num_positive': positive_count  # Number of positive instances in the selection\n",
        "        })\n",
        "\n",
        "        # Remove the selected instances from X and Y\n",
        "        X = np.delete(X, selected_indices, axis=0)\n",
        "        Y = np.delete(Y, selected_indices, axis=0)\n",
        "\n",
        "    return total_selected\n",
        "\n",
        "# Example usage\n",
        "selected_subsets = evaluate_policy_with_removal(policy_net, X, Y)\n",
        "\n",
        "# Output should include the number of selected instances and how many of them are positive\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
